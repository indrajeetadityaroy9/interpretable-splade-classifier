## Goodhart’s Law Applies to NLP’s Explanation Benchmarks

Jennifer Hsia _[†∗]_ Danish Pruthi _[‡]_ Aarti Singh _[†]_ Zachary C. Lipton _[†]_


_†_ Carnegie Mellon University, Pittsburgh, PA

_‡_ Indian Institute of Science, Bangalore
_∗_
jhsia2@cs.cmu.edu


**Abstract**


Despite the rising popularity of saliency-based _explanations_, the research community remains at an
impasse, facing doubts concerning their purpose, efficacy, and tendency to contradict each other. Seeking to
unite the community’s efforts around common goals, several recent works have proposed evaluation metrics.
In this paper, we critically examine two sets of metrics: the ERASER metrics ( _comprehensiveness_ and
_sufficiency_ ) and the EVAL-X metrics, focusing our inquiry on natural language processing. First, we show
that we can inflate a model’s comprehensiveness and sufficiency scores dramatically _without altering its_
_predictions or explanations on in-distribution test inputs_ . Our strategy exploits the tendency for extracted
_explanations_ and their complements to be “out-of-support” relative to each other and in-distribution inputs.
Next, we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple method that encodes
the label, even though EVAL-X is precisely motivated to address such exploits. Our results raise doubts
about the ability of current metrics to guide explainability research, underscoring the need for a broader
reassessment of what precisely these metrics are intended to capture.

### **1 Introduction**


Popular methods for “explaining” the outputs of _natural language processing_ (NLP) models operate by
highlighting a subset of the input tokens that ought, in some sense, to be _salient_ . The community has initially
taken an _ad hoc_ approach to evaluating these methods, looking at select examples to see if the highlighted
tokens align with intuition. Unfortunately, this line of research has exhibited critical shortcomings [1].
Popular methods tend to disagree substantially in their highlighted token _explanations_ [2, 3]. Other methods
highlight tokens that simply encode the predicted label, rather than offering additional information that could
reasonably be called an _explanation_ [4]. This state of affairs has motivated an active area of research focused
on developing evaluation metrics to assess the quality of such _explanations_, focusing on such high-level
attributes as _faithfulness_, _plausibility_, and _conciseness_, among others.


In particular, _faithfulness_ has emerged as a primary focus of explainability metrics. According to Jacovi and
Goldberg [5], faithfulness “refers to how accurately [an explanation] reflects the true reasoning process of
the model.” Such metrics are typically concerned with how a model’s output changes when the model is
invoked with only the _explanatory_ tokens or when the model receives the non-explanatory tokens [6–13].
Unfortunately, these token subsets do not in general resemble natural documents. Informally, we might say
that we are _explaining_ the model by considering its behavior exclusively on examples that lie outside the
support of the distribution of interest [14]. This raises concerns about whether degradation in performance
absent the ostensibly salient features could be due merely to distribution shift [9]. This focus on feature


1


Figure 1: The ERASER benchmark’s quantitative metrics depend on the prediction model’s _confidence_ when
invoked with the original input, the explanation features, and non-explanation features. In this example
movie review, we illustrate how our meta-algorithm can inflate comprehensiveness scores without altering
predictions or _explanations_ on the original inputs. Our technique exploits the fact that _explanation-only_
features and _non-explanation_ features are identifiably different.


subsets in evaluation is no accident; many _explanation_ algorithms determine saliency in the first place by
observing changes in model outputs on different feature subsets [15, 16].


In this paper, we investigate two sets of _explanation_ metrics that rely on evaluating the model on masked
inputs: the ERASER metrics (i.e. comprehensiveness and sufficiency) and the EVAL-X metrics. We introduce
simple algorithms that wrap existing predictors, achieving near-optimal scores on these faithfulness metrics
_without_ doing anything that a reasonable practitioner might describe as providing better _explanations_ . In
the ERASER benchmark’s case, we use a simple wrapper model to inflate the faithfulness scores of a
given prediction model and saliency method _while_ generating identical predictions and _explanations_ on the
test set. We achieve this by assigning different model behaviors to the masked inputs used in faithfulness
evaluation and the original inputs used in prediction and _explanation_ generation (Figure 1). The second
set of metrics, from EVAL-X, is advertised as a way to detect when models encode predictions in their
explanations. Optimizing for these metrics is claimed to produce “high fidelity/accuracy explanations without
relying on model predictions generated by out-of-distribution input” [4]. Nevertheless, we show that two
simple model-agnostic encoding schemes can achieve optimal scores, undercutting the very motivation of the
EVAL-X metrics.


While benchmarks seldom capture all desiderata related to underlying tasks of interest, significant progress
on a well-designed benchmark should at least result in useful technological progress. Unfortunately, our
results suggest that these metrics fail to meet this bar. Rather, they exemplify Goodhart’s law: once optimized,
they cease to be useful. While our results should raise alarms, they do not necessarily doom the enterprise of
designing metrics worth optimizing. Often, first attempts at technical definitions exhibit a speculative flavor,


2


serving as tentative proposals inviting an iterative process of community scrutiny and further refinement. We
might look to the development of differential privacy after years of alternate proposals as inspiration. That
said, our results demonstrate considerable challenges that must be overcome to produce coherent objectives
to guide explainability research.

### **2 Related Work**


**Evaluating Explanations .** One desideratum of saliency methods is _faithfulness_ or _fidelity_, described as the
ability to capture the “reasoning process” behind a model’s predictions [5, 17]. Ribeiro et al. [15] claim that
a saliency method is faithful if it “correspond[s] to how the model behaves in the vicinity of the instance
being predicted”. This work has inspired a wave of removal-based metrics that measure the faithfulness of a
saliency method by evaluating the model on _neighboring instances_, created by perturbing or removing tokens.
These removal-based metrics can be broadly categorized into: (i) metrics that assess model behavior on the
_explanation_ features alone; and (ii) metrics that assess model performance on the input features excluding the
_explanation_ features. The first category expresses the intuition that “faithful” attributions should comprise
features _sufficient_ for the model to make the same prediction with high confidence. Our experiments focus on
optimizing for a metric called sufficiency [6], but other similar metrics include prediction gap on unimportant
feature perturbation [7], insertion [8], and keep-and-retrain [9]. The second category expresses the notion that
the selected features are _necessary_ . The metric used in our experiments is called comprehensiveness [6], while
many other variations have been proposed, including prediction gap on important feature perturbation [7],
deletion [8], remove and retrain [9], JS divergence of model output distributions [10], area over perturbation
curve [12], and switching point [13]. Notably, Jethani et al. [4] are less concerned with “explaining the
model” and more concerned with justifying the label; their evaluation checks the behavior of, EVAL-X, an
independent evaluator model (not the original predictor), when invoked on the _explanation_ text.


**The “Out-of-Support” Issue .** One issue has emerged to reveal critical shortcomings in these current
approaches to saliency: they attempt to “explain” a model’s behavior on some population of interest (e.g.,
natural documents) by evaluating how the model behaves on a wildly different population (the documents that
result from masking or perturbing the original documents) [9, 18]. Among proposed patches, Hooker et al. [9]
create modified training and test sets by removing the most important features according to their attribution
scores, then retraining and evaluating the given model on the modified datasets. While such patches address a
glaring flaw, we still lack an affirmative argument for their usefulness; the OOD issue reveals a fundamental
problem that does not necessarily resolve when the OOD issue is patched. Moreover, the retrained model is
no longer the object of interest that we sought to explain in the first place. Others have tried to bridge the
distribution gap by modifying only the training distribution. Hase et al. [14] suggest modifying the training
set by adding randomly masked versions of each training instance, thus all masked inputs would technically
be in-distribution. Although Hase et al. [14] mention that it is possible to game metrics when the masked
samples are out-of-distribution, they do not demonstrate this. We offer concrete methods to demonstrate not
only _how easy_ it is to optimize removal-based faithfulness metrics, but also _how much_ these metrics can be
optimized. Following a related idea, Jethani et al. [4] introduce an evaluator model EVAL-X that is trained on
randomly masked inputs from the training data. Their metrics consist of the EVAL-X’s accuracy and AUROC
when invoked on _explanation-only_ inputs. While the authors claim that EVAL-X can distinguish between
extract-then-classify models that encode and those that do not, we demonstrate two encoding methods that
are scored optimally by EVAL-X, revealing a critical shortcoming.


**Manipulating Explanations .** Slack et al. [18] demonstrate how one could exploit the OOD issue to


3


manipulate the feature importance ranking from LIME and SHAP and conceal problems vis-a-vis fairness.
They propose an adversarial wrapper classifier designed such that a sensitive feature that the model truly relies
on will not be detected as the top feature. Pruthi et al. [19] demonstrate the manipulability of attention-based
_explanations_ and Wang et al. [20] the manipulability of gradient-based _explanations_ in the NLP domain.
Many have also explored the manipulability of saliency methods but in the image domain [21–23]. In a
more theoretical work, Anders et al. [24] use differential geometry to establish the manipulability of popular
saliency methods. **Key difference:** while these works are concerned with manipulating the _explanations_
themselves, we are concerned with manipulating the leaderboard.

### **3 Optimizing the ERASER Benchmark Metrics**


Figure 2: Our meta-algorithm, which wraps a prediction model and saliency method, applied to a movie
review in a sentiment analysis task. First, our case detector determines whether the input consists of (Left)

probabilities output by the original prediction model. Otherwise, our meta-algorithm manipulates the model
confidence to inflate the sufficiency and comprehensiveness scores.


Let _x_ denote a sequence of input tokens, _y ∈{_ 1 _, . . ., |Y|}_ a categorical target variable, and _f_ a prediction
model that maps each input to a predicted probability over the _|Y|_ labels. By ˆ _y_, we denote the predicted label,
and ˆ _e_ a generated _explanation_ consisting of an ordered subset of the tokens in _x_ . By _x \_ ˆ _e_, we denote the
_non-explanation_ features that result from deleting the _explanation_ .


**Definition 1** (Sufficiency) **.** _Sufficiency is the difference between the model confidence (on the predicted label)_


4


_given only the_ explanation _features and the model confidence given the original input:_


_f_ ( _Y_ = ˆ _y|X_ = ˆ _e_ ) _−_ _f_ ( _Y_ = ˆ _y|X_ = _x_ ) _._ (1)


Note that our definition is a negation of the original sufficiency metric [6]. We make this change for notational
convenience and to reflect the intuition that sufficiency is a positive attribute: higher sufficiency should be
better.


**Definition 2** (Comprehensiveness) **.** _Comprehensiveness is the difference between the model confidence given_
_the_ non-explanation _features and the model confidence given the original input:_


_f_ ( _Y_ = ˆ _y|X_ = _x_ ) _−_ _f_ ( _Y_ = ˆ _y|X_ = _x \_ ˆ _e_ ) _._ (2)


Intuitively, a higher comprehensiveness score is thought to be better because it suggests the _explanation_
captures most of the “salient” features, making it difficult to predict accurately in its absence.


For a given prediction model and saliency method, we aim to increase the sufficiency and comprehensiveness
scores while preserving the original predictions and _explanations_ . Let the model confidence in the original
inputs be _f_ ( _Y_ = ˆ _y|X_ = _x_ ) = _c_ . Then, sufficiency has a range of [ _−c,_ 1 _−_ _c_ ], and is maximized when
we set _f_ ( _Y_ = ˆ _y|X_ = ˆ _e_ ) to 1. Comprehensiveness has a range of [ _c −_ 1 _, c_ ], and is maximized when we
set _f_ ( _Y_ = ˆ _y|X_ = _x \_ ˆ _e_ ) to 0. However, there is a tradeoff between these two metrics, where the two
metrics depend on _c_ in opposite directions. To maximize sufficiency, we must minimize _c_, for which the
lowest possible value approaches 1 _/|Y|_ (any lower and we change the predicted class). On the other hand, to
maximize comprehensiveness, we must maximize _c_ . The upshot of this tradeoff is that the sum of sufficiency
and comprehensiveness scores lies in the range [ _−_ 1 _,_ 1] and thus cannot exceed 1.


**3.1** **Method**


The key to our method for maximizing these scores begins with the insight that _explanation-only_ inputs ˆ _e_ and
_non-explanation_ inputs _x \_ ˆ _e_ are easy to distinguish from original inputs _x_ . Thus, by recognizing which case
we face, our model can output strategically chosen confidence scores that inflate the resulting faithfulness
scores. To instantiate this idea, we implement a case detector, trained to recognize whether an input is (i)
an original input _x_ ; (ii) the _explanation-only_ features for a particular label; or (iii) the _explanation-removed_
features for a particular label. As a result, our case detector must choose among 2 _|Y|_ + 1 cases where
_|Y|_ is the number of classes. For any (prediction model, saliency method) pair, we must train a fresh case
predictor.. Given such a pair, we construct a training set that consists of every instance in the original train
set, the _explanation-only_ features for that instance, and the _non-explanation_ features for that instance. The
corresponding labels are produced straightforwardly, e.g., “an explanation-only input whose predicted label
was class _j_ ”.


Our **meta-algorithm** wraps the original predictor as follows (Figure 2): if the detected case is original, we
run the input through the original model, thereby preserving the same prediction ˆ _y_ and _explanation_ ˆ _e_ . If the
detected case is _explanation_ features for label _y_, we manually set the model confidence to 1 for label _y_, and
0 for the other labels. If the detected case is _explanation-removed_ features for a label _y_, we set the model
confidence to 0 for label _y_, and 1 for a random other label _̸_ = _y_ . If the case predictor is perfectly accurate,
this procedure achieves a sufficiency score of 1 _−_ _c_ and the comprehensiveness score _c_, reaching Pareto
optimality.


5


|Col1|Movies|BoolQ|
|---|---|---|
|**Method**|**F1 Score**<br>**Comp.**<br>**Suff.**<br>**Comp.+Suff.**|**F1 Score**<br>**Comp.**<br>**Suff.**<br>**Comp.+Suff.**|
|Attention<br>+ meta-algo|92.4<br>0.18<br>-0.11<br>0.07<br>92.4<br>**0.89**<br>**-0.09**<br>**0.80**|58.4<br>0.05<br>-0.01<br>0.04<br>58.4<br>**0.59**<br>**0.16**<br>**0.75**|
|IG<br>+ meta-algo|92.4<br>0.26<br>**-0.08**<br>0.18<br>92.4<br>**0.83**<br>-0.09<br>**0.74**|58.4<br>0.03<br>0.00<br>0.04<br>58.4<br>**0.73**<br>**0.25**<br>**0.98**|
|LIME<br>+ meta-algo|92.4<br>0.38<br>-0.01<br>0.37<br>92.4<br>**0.82**<br>**0.00**<br>**0.82**|58.4<br>0.09<br>0.08<br>0.16<br>58.4<br>**0.73**<br>**0.26**<br>**1.00**|
|Random<br>+ meta-algo|92.4<br>0.01<br>-0.06<br>-0.05<br>92.4<br>**0.65**<br>**0.12**<br>**0.77**|58.4<br>0.01<br>-0.06<br>-0.05<br>58.4<br>**0.65**<br>**0.12**<br>**0.77**|


Table 1: We demonstrate the comprehensiveness (comp.) and sufficiency (suff.) gains of our meta-algorithm
on the ERASER Benchmark’s Movies and BoolQ datasets. We maintain the same predictions on the original
inputs, hence there are no changes in the F1 score. At the same time, on the Movies dataset, we achieve a
0 _._ 59 gain in comprehensiveness, and 0 _._ 05 gain in sufficiency, when averaged across these model-saliency
method pairs. On the BoolQ dataset, we achieve a 0 _._ 63 average comprehensiveness gain and 0 _._ 20 average
sufficiency gain.


**3.2** **Experimental Setup**


We assess the efficacy of our meta-algorithm for inflating the sufficiency and comprehensiveness metrics
using the same datasets as in the original ERASER benchmark paper [6]. We present the results for the
Movies [25] and BoolQ [26] datasets in the main paper and share the remaining results for other datasets
including Evidence Inference [27], FEVER [28], and MultiRC [29] in the Appendix (Tables 2 and 3).


We use pre-trained BERT tokenizers and models [30] for the case detectors and the prediction models. We
train the prediction models for 10 epochs and the case detector models for 3 epochs, both with a batch size
of 32, and a learning rate of 2e _−_ 5. We experiment with several saliency methods, including LIME [15],
Integrated Gradients (IG) [31], Attention [32], and a random baseline (which randomly highlights tokens).
For each saliency method, we use the top 10% of the input features with the highest attribution scores as the
_explanation_ . We train a different case detector for each prediction model and saliency method pair. We use a
macro-averaged F1 score for the prediction model’s task performance and comprehensiveness and sufficiency
for faithfulness.


**3.3** **Results**


Across all the investigated setups, our meta-algorithm is effective in increasing the comprehensiveness
and sufficiency scores. For instance, on the Movies dataset, with attention-based _explanations_ the initial
comprehensiveness score was 0 _._ 18, but we inflate it to 0 _._ 89 (Table 1). Similarly, on the BoolQ dataset, for
the IG method, we again see a dramatic increase, from 0 _._ 03 to 0 _._ 73. On average, on the Movies dataset, our
meta-algorithm has a comprehensiveness gain of 0 _._ 59 and a sufficiency gain of 0 _._ 05. Similarly, on the BoolQ
dataset, our meta-algorithm’s average comprehensiveness gain is 0 _._ 63 and sufficiency gain is 0 _._ 20. To put
these gains in perspective, recall that the sum of comprehensiveness and sufficiency cannot exceed 1.


As one may note, the comprehensiveness gains are larger than the sufficiency gains. This is because the
headroom for comprehensiveness gains exceeds that for sufficiency gain in practice. The comprehensiveness
gains are bounded by how close the original confidence scores are to 0% for _non-explanation_ features. In


6


Figure 3: We compare the model confidence in _explanation_ and _non-explanation_ features from the original model and our meta-algorithm on the Movies dataset. (Left): The optimal comprehensiveness is
achieved when the model confidence in _non-explanation_ features is 0%. Since the original confidence in
_non-explanation_ features is high (77.7% on average), there is a large room to deflate the confidence for
comprehensiveness gain. In practice, our meta-algorithm method achieves _<_ 5% average confidence, which is
close to optimal. (Right): The optimal sufficiency is achieved when the model confidence in _non-explanation_
features is 100%. Since the original model’s confidence in _explanation_ features is already high (85.8% on
average), there is little room to inflate it for sufficiency gain. In practice, our meta-algorithm achieves 100%
confidence.


practice, on the Movies dataset, we observe that the original confidence for _non-explanation_ features is 77.7%
(far from 0%), indicating a large potential for score improvement (Fig. 3). On the other hand, the room for
inflating sufficiency is capped by how close the original confidence scores for _explanation_ features are to
100%. For the Movies dataset, the original model confidence for _explanation_ features is 85.8% (close to
100%), indicating a smaller potential for score improvement (Fig. 3).


Using our meta-algorithm, we minimize the average model confidence for _non-explanation_ features to 1.6%
(close to the optimal 0%) and maximize the confidence for _explanation_ features to the optimal 100%. We also
compare the sum of the comprehensiveness and sufficiency scores in the last column of Table 2. For any given
prediction model and saliency method pair, our meta-algorithm shows substantial gains in faithfulness sum
score. On average, on the Movies dataset, our meta-algorithm’s sum faithfulness score is 0.78, whereas the
underlying method’s faithfulness sum score is 0.14. On BoolQ, our meta-algorithm’s faithfulness sum score
is 0.88 whereas the underlying method’s score is 0.05. In some instances, we even achieve the exact optimal
score of 1, as seen when our meta-algorithm is applied with LIME for BoolQ. The main reason why our
scores are not always 1 is that our case detector does not always have perfect test accuracy (Table 3).


If one took these scores at face value, our improved faithfulness scores would appear to suggest that the
_explanations_ from our meta-algorithm are substantially more faithful than the _explanations_ from the original,
non-optimized methods. However, on original (not masked) inputs, we typically output the same predictions
and _explanations_ as the original models. Our ability to max out these benchmarks without even changing
the explanations themselves (on the population of interest) suggest that these metrics are not suited to guide
advances in explainability research.


Another alarming observation is that our optimized version of **random** _**explanations**_ has higher faithfulness
scores than the non-optimized version of the other saliency methods. A random _explanation_ is generated
without interaction with the prediction model, so one would typically expect it to be less faithful than other


7


proposed saliency methods. However, using our meta-algorithm, the random _explanations_ achieve higher
faithfulness scores, raising further doubts about the reasonableness of these scores.

### **4 Optimizing scores on EVAL-X Metrics**


The EVAL-X metrics are focused on the extract-then-classify variety of “explainable” classifiers. They
confront the issue that when an _explanation_ extractor and label predictor are trained jointly, the extractor may
end up doing all of the work by simply “encoding" the eventual prediction, rather than providing evidence.
Consider for instance, on a binary classification task, an _explanation_ extractor that outputs a period whenever
the prediction is positive, and a comma whenever the prediction is negative. The classifier can perfectly
recover the predicted label from the single token, encoded _explanation_ . This issue has been highlighted in
several past works, including [33] and [2]. To detect such encoded _explanations_, Jethani et al. [4] propose
evaluating the _explanations_ under the “true” posterior of the predicted label. Concretely, what they propose is
to train an _evaluator model_ called EVAL-X. During training, EVAL-X is given training examples consisting
of randomly masked inputs together with the true corresponding target. The proposed evaluation strategy for
any explanation method is to assess the accuracy ( **eACC** ) and AUC ( **eAUROC** ) of the EVAL-X model on
the masked input consisting only of _explanation_ tokens. Jethani et al. [4] suggest that encoding in extractthen-classify models should be detectable as a sharp decline in eACC and eAUROC compared to ACC and
AUROC. We define a sharp decline as a 10% drop.


**4.1** **Method**


To game this metric, we propose two simple encoding methods that achieve high eACC and eAUROC (close
to ACC and AUROC). Formally, if the _explanation_ is generated using only the predicted label and no other
access to the prediction model, then the _explanation_ encodes the prediction. Given such an _explanation_, the
extract-then-classify model can then confidently map the _explanation_ to the encoded label by accessing a
fixed code book.


**Method 1: Token-label Likelihood Ratio** For each label _Y_ and token _T_ in the training set, we calculate
the frequency with which the token _T_ appears among instances associated with the label _Y_ . At test time,
for a given input, we first use the prediction model to get ˆ _y_ . Then, to generate an encoded _explanation_, we
output the _k_ input tokens that have the highest likelihood ratio (token frequency under label ˆ _y_ divided by
token frequency under labels _Y ̸_ = ˆ _y_ ).


**Method 2: Evaluator Model Queries** Alternatively, if we know the exact weights of the evaluator model,
EVAL-X, then optimizing the metric scores is trivial. At test time, for a given input, we first use the prediction
model to get ˆ _y_ . Then, we can simply query the evaluator model and output the encoded _explanation_ as any
one input token for which the evaluator model outputs ˆ _y_ . Even without access to the exact model weights of
EVAL-X, we can train multiple approximations of EVAL-X with different random seeds, then use majority
voting across the approximation models to output the final _explanations_ and predictions.


**4.2** **Setup**


Here, we describe the setup for evaluating the efficacy of the two encoding methods for the EVAL-X metrics
on the Movies dataset [25, 6]. We use pre-trained BERT tokenizers and models for the prediction model


8


and train it for 10 epochs with a batch size of 32 and a learning rate of 2e-5. We compute standard ACC
and AUROC and the EVAL-X metric versions (i.e. eACC and eAUROC). For the first encoding method,
token-label correlation, we average the results over five random seeds of the evaluator model. For the second
encoding method, we train one evaluator model and four approximation models of different random seeds,
then use majority voting to combine the predictions and _explanations_ .


**4.3** **Results**


Figure 4: These two figures evaluate whether our two encoding methods—Token-Label Likelihood Ratio
(TL) and Majority-Voting (MV)—are detected as encoding methods by the EVAL-X Metrics. We vary the
length of the encoded _explanations_ along the x-axis. If a method’s eACC and eAUORC are much lower (more
than 10% drop) than ACC and AUROC, then it means that the method has been detected as an encoding
method by the EVAL-X metrics. We evaluate our eACC (left) and eAUROC (right) against their encoding
cutoffs in dashed lines. From token lengths 10 to 100, TL _explanations_ ’ eACC and eAUROC are both above
the encoding cutoff, meaning TL has not been detected as an encoding method. For all token lengths, MV
_explanations_ ’ eAUROC are above the cutoff, while its eACC eventually increases to above the encoding
cutoff starting from 50 tokens.


We evaluate our two encoded saliency methods on the Movies dataset. Our methods achieve eACC and
eAUROC above the encoding cutoff, meaning within a 10% drop of the ACC and eAUROC. This indicates
that our methods have not been detected as encoded saliency methods by the EVAL-X metrics.


**Method 1: Token-label Likelihood Ratio** We encode the predictions into _explanations_ using token-label
likelihood ratio. The resulting eACC and eAUROC are both above the encoding cutoff of ACC and AUROC
across varying _explanation_ lengths from 10 to 100 (Fig. 4). On the Movies dataset, with a length of 10 tokens,
our encoded _explanations_ ’ eACC is already above the encoding cutoff at a high of 83.7%. As we increase the
encoded _explanation_ length, eACC only increases till it matches ACC exactly at 92.5%. The success of this
encoding method shows how easy it is to score high on the EVAL-X metrics with encoded _explanations_ that
are constructed completely independently of interactions with the prediction model (other than accessing the
predicted labels on the original inputs).


**Method 2: Evaluator Model Queries** Using direct access to the evaluator model, we can select any single
token in a given input that results in the evaluator model predicting the label we wish to encode. The resulting


9


eACC and eAUROC would match ACC and AUROC exactly. This contrasts directly with the metric’s original
motivation, where they claim a single feature, encoded _explanation_ could easily be detected as encoded.
Although it may be the case that a _random_ single input feature can be detected by their metric, a single feature
encoded by accessing the evaluator model can avoid being detected.


We then consider the scenario where we do not have direct access to the evaluator model. In this case, we
can train several approximations of the evaluator model. This is possible since the training scheme is simple
and the data is the training set of our original prediction model. The resulting, majority-voted _explanations_
achieve eACC and eAUROC above the encoding cutoff starting from a length of 50 tokens (Figure 4). These
results demonstrate that it can be easy to trivially optimize for a metric that relies on an easily accessible or
approximated evaluator model.

### **5 Conclusion**


We have demonstrated that simple methods can achieve substantially better and, sometimes, near-optimal
scores on current metrics for evaluating rationales _without_ producing rationales that anyone would reasonably
claim as being more faithful. While these metrics represent honest efforts to codify desiderata associated
with such rationales, we conclude that they are not suitable to function as benchmarks. In general, few
metrics capture all desiderata of interest. Accuracy does not capture all desiderata associated with image
classification and ROUGE score is a weak proxy for translation quality. However, for a quantitative metric to
function as a useful benchmark, it must be the case that concerted efforts to optimize this metric necessarily
bring about desired technological improvements. The effort to lower ImageNet error truly required genuine
advancements in computer vision and efforts to increase ROUGE have revolutionized machine translation.
Efforts to optimize a metric, respecting the rules of the game should not be regarded as mere “gaming”;
inspiring such efforts is the very purpose of a benchmark. In general, when developing a metric, it often
takes multiple iterations of proposals and criticisms to arrive at a useful formalism. For example, in privacy,
many formal notions of privacy were proposed, and each in turn criticized, before the community arrived at
the robust and mathematically rigorous measure of differential privacy. Likewise, many ways to quantify
information were proposed before Shannon’s seminal work. While the common word _explanation_ may be
hopelessly broad, we do not rule out the possibility that measures might be proposed that rigorously capture
some useful notion of _saliency_ . Our hope is that these results can inspire improved definitions capable of
guiding methodological research.

### **Acknowledgements**


The authors gratefully acknowledge support from the NSF (FAI 2040929 and IIS2211955), UPMC, Highmark
Health, Abridge, Ford Research, Mozilla, the PwC Center, Amazon AI, JP Morgan Chase, the Block Center,
the Center for Machine Learning and Health, NSF CIF grant CCF1763734, the AI Research Institutes program
supported by NSF and USDA-NIFA under award 2021-67021-35329, and the CMU Software Engineering
Institute (SEI) via Department of Defense contract FA8702-15-D-0002.

### **References**


[1] Zachary C Lipton. The mythos of model interpretability. _Communications of the ACM (CACM)_, 2018.


10


[2] Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C
Lipton, Graham Neubig, and William W Cohen. Evaluating explanations: How much do explanations
from the teacher aid students? _Transactions of the Association for Computational Linguistics_, 10:
359–375, 2022.


[3] Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu
Lakkaraju. The disagreement problem in explainable machine learning: A practitioner’s perspective.
_arXiv preprint arXiv:2202.01602_, 2022.


[4] Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have we learned
to explain?: How interpretability methods can learn to encode predictions in their interpretations. In
_International Conference on Artificial Intelligence and Statistics_, pages 1459–1467. PMLR, 2021.


[5] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define
and evaluate faithfulness? _arXiv preprint arXiv:2004.03685_, 2020.


[6] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher,
and Byron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. _arXiv preprint_
_arXiv:1911.03429_, 2019.


[7] Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri,
Marinka Zitnik, and Himabindu Lakkaraju. Openxai: Towards a transparent evaluation of model
explanations. _Advances in Neural Information Processing Systems_, 35:15784–15799, 2022.


[8] Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of
black-box models. _arXiv preprint arXiv:1806.07421_, 2018.


[9] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability
methods in deep neural networks. _Advances in neural information processing systems_, 32, 2019.


[10] Sofia Serrano and Noah A Smith. Is attention interpretable? _arXiv preprint arXiv:1906.03731_, 2019.


[11] Ian C Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model
explanation. _The Journal of Machine Learning Research_, 22(1):9477–9566, 2021.


[12] Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Bach, and Klaus-Robert Müller.
Evaluating the visualization of what a deep neural network has learned, 2015.


[13] Dong Nguyen. Comparing automatic and human evaluation of local explanations for text classification.
In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-_
_putational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1069–1078,
2018.


[14] Peter Hase, Harry Xie, and Mohit Bansal. The out-of-distribution problem in explainability and search
methods for feature importance explanations. _Advances in neural information processing systems_, 34:
3650–3666, 2021.


[15] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the
predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on_
_knowledge discovery and data mining_, pages 1135–1144, 2016.


[16] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. _Advances in_
_neural information processing systems_, 30, 2017.


11


[17] Chun Sik Chan, Huanqi Kong, and Guanqing Liang. A comparative study of faithfulness metrics for
model interpretability methods. _arXiv preprint arXiv:2204.05514_, 2022.


[18] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling lime
and shap: Adversarial attacks on post hoc explanation methods. In _Proceedings of the AAAI/ACM_
_Conference on AI, Ethics, and Society_, pages 180–186, 2020.


[19] Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. Learning to
deceive with attention-based explanations. In _Annual Conference of the Association for Computational_
_Linguistics (ACL)_, July 2020.


[20] Junlin Wang, Jens Tuyls, Eric Wallace, and Sameer Singh. Gradient-based analysis of nlp models is
manipulable. _arXiv preprint arXiv:2010.05419_, 2020.


[21] Juyeon Heo, Sunghwan Joo, and Taesup Moon. Fooling neural network interpretations via adversarial
model manipulation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.


[22] Ann-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, Klaus-Robert
Müller, and Pan Kessel. Explanations can be manipulated and geometry is to blame. _Advances in neural_
_information processing systems_, 32, 2019.


[23] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In
_Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 3681–3688, 2019.


[24] Christopher Anders, Plamen Pasliev, Ann-Kathrin Dombrowski, Klaus-Robert Müller, and Pan Kessel.
Fairwashing explanations with off-manifold detergent. In _International Conference on Machine Learn-_
_ing_, pages 314–323. PMLR, 2020.


[25] Omar Zaidan and Jason Eisner. Modeling annotators: A generative approach to learning from annotator
rationales. In _Proceedings of the 2008 conference on Empirical methods in natural language processing_,
pages 31–40, 2008.


[26] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. _arXiv preprint_
_arXiv:1905.10044_, 2019.


[27] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C Wallace. Inferring which medical treatments
work from reports of clinical trials. _arXiv preprint arXiv:1904.01606_, 2019.


[28] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale
dataset for fact extraction and verification. _arXiv preprint arXiv:1803.05355_, 2018.


[29] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond
the surface: A challenge set for reading comprehension over multiple sentences. In _Proceedings of_
_the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:_
_Human Language Technologies, Volume 1 (Long Papers)_, pages 252–262, 2018.


[30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.


[31] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
_International conference on machine learning_, pages 3319–3328. PMLR, 2017.


12


[32] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention.
In _International conference on machine learning_, pages 2048–2057. PMLR, 2015.


[33] Marcos V Treviso and André FT Martins. The explanation game: Towards prediction explainability
through sparse communication. _arXiv preprint arXiv:2004.13876_, 2020.


13


### **A Additional Results for Sufficiency and Comprehensiveness**

We show our faithfulness optimization results in Table 2 and case detection accuracy in Table 3 for datasets:
Evidence Inference [27], BoolQ [26], Movies [25], MultiRC [29], and FEVER [28]).


14


Table 2: Gaming ERASER’s Sufficiency and Comprehensiveness


**F1 Score** **Comp.** **Suff.** **Comp.+Suff.**
**Evidence Inference**
Attention 58.2 0.13 -0.15 -0.02
Attention + meta-algo 58.2 0.61 -0.08 0.54
Gradient 58.3 0.15 -0.12 0.04
Gradient + meta-algo 58.3 0.61 -0.10 0.51
LIME 58.2 0.16 -0.15 0.01
LIME + meta-algo 58.2 0.66 0.14 0.79
Random 58.2 0.05 -0.21 -0.16
Random + meta-algo 58.2 0.65 -0.15 0.50
**BoolQ**
Attention 58.4 0.05 -0.01 0.04
Attention + meta-algo 58.4 0.59 0.16 0.75
Gradient 58.4 0.03 0.00 0.04
Gradient + meta-algo 58.4 0.73 0.25 0.98
LIME 58.4 0.09 0.08 0.16
LIME + meta-algo 58.4 0.73 0.26 1.00
Random 58.4 0.01 -0.06 -0.05
Random + meta-algo 58.4 0.65 0.12 0.77
**Movies**
Attention 92.4 0.18 -0.11 0.07
Attention + meta-algo 92.4 0.89 -0.09 0.80
Gradient 92.4 0.26 -0.08 0.18
Gradient + meta-algo 92.4 0.83 -0.09 0.74
LIME 92.4 0.38 -0.01 0.37
LIME + meta-algo 92.4 0.82 0.00 0.82
Random 92.4 0.01 -0.06 -0.05
Random + meta-algo 92.4 0.65 0.12 0.77
**MultiRC**
Attention 71.4 0.28 -0.16 0.11
Attention + meta-algo 70.3 0.68 -0.18 0.50
Gradient 71.4 0.26 -0.23 0.04
Gradient + meta-algo 70.7 0.68 -0.20 0.48
LIME 71.4 0.31 -0.23 0.07
LIME + meta-algo 71.0 0.77 -0.04 0.73
Random 71.4 0.10 -0.39 -0.29
Random + meta-algo 71.4 0.75 -0.29 0.47
**FEVER**
Attention 90.7 0.13 -0.15 -0.02
Attention + meta-algo 90.7 0.61 -0.08 0.54
Gradient 90.7 0.15 -0.12 0.04
Gradient + meta-algo 89.2 0.61 -0.10 0.51
LIME 90.7 0.09 -0.23 -0.14
LIME + meta-algo 90.0 0.91 -0.06 0.85
Random 90.7 0.04 -0.24 -0.21
Random + meta-algo 90.0 0.91 -0.15 0.75

15


Table 3: ERASER Case detector accuracy


Case detector Accuracy (%)
**Evidence Inference**
Attention 78.6
Gradient 77.5
LIME 88.9
Random 78.6
**BoolQ**
Attention 91.8
Gradient 99.3
LIME 99.8
Random 92.2
**Movies**
Attention 93.3
Gradient 91.2
LIME 93.7
Random 85.0
**MultiRC**
Attention 82.6
Gradient 81.7
LIME 90.9
Random 82.3
**FEVER**
Attention 93.1
Gradient 91.6
LIME 90.7
Random 91.5

### **B Additional Results for EVAL-X Metrics**


We include the label recovery rate, ACC, AUROC, eACC, and eAUROC for encoding method 1 (Token-label
Likelihood Ratio) in Table 4 and for encoding method 2 (Majority Voting of Evaluator Model Approximations)
in Table 5 on the Movies dataset [25] in the ERASER benchmark [6].


For method 2 (Evaluator Model Queries), we compare using majority-voting of four evaluator model
approximations to using only a single evaluator model approximation in Table 5 and Table 6. We find that
the EVAL-X scores are lower and have a higher variance when using a single approximation model. For the
single evaluator model approximation experiments, we use one seed for the approximate model and four
random seeds for the evaluator model.


16


Table 4: EVAL-X Encoding Method 1: Naive Bayes Method


Num. of tokens Label recovery rate (%) ACC (%) eACC (%) AUROC eAUROC


1 100.0 92.5 0.615 _±_ 0.064 0.925 0.692 _±_ 0.111
5 100.0 92.5 0.776 _±_ 0.065 0.925 0.865 _±_ 0.037
10 100.0 92.5 0.837 _±_ 0.054 0.925 0.912 _±_ 0.014
20 100.0 92.5 0.894 _±_ 0.026 0.925 0.931 _±_ 0.013
50 100.0 92.5 0.917 _±_ 0.012 0.925 0.929 _±_ 0.012
100 100.0 92.5 0.924 _±_ 0.002 0.925 0.935 _±_ 0.008


Table 5: EVAL-X Encoding Method: Majority Voting of Evaluator Model Approximations


Num. of tokens Label recovery rate (%) ACC (%) eACC (%) AUROC (%) eAUROC (%)


1 95.5 89.0 84.0 93.7 93.0
10 100.0 92.5 80.0 92.0 91.6
50 100.0 92.5 83.0 91.6 90.7
70 100.0 92.5 87.5 92.5 91.5
100 100.0 92.5 91.0 91.3 92.5


Table 6: EVAL-X Encoding Method: Single Evaluator Model Approximation


Num. of tokens Label recovery rate (%) ACC (%) eACC (%) AUROC (%) eAUROC (%)


1 98.1 ± 2.4 90.9 ± 2.0 82.1 ± 11.0 90.9 ± 2.0 90.5 ± 2.5
5 99.1 ± 0.4 91.6 ± 0.4 80.9 ± 13.3 91.6 ± 0.4 87.4 ± 7.7
10 99.2 ± 0.6 91.7 ± 0.6 80.9 ± 13.3 91.7 ± 0.6 86.5 ± 7.6
50 98.7 ±1.3 91.5 ± 1.5 83.3 ± 10.8 91.5 ± 1.5 90.1 ± 4.7
70 99.2 ± 0.8 92.0 ± 0.5 83.1 ± 10.7 92.9 ± 0.5 91.0 ± 3.5
100 98.5 ± 2.1 91.3 ± 1.6 83.4 ± 10.1 91.2 ± 1.6 91.3 ± 3.6


17


