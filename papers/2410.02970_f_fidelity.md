Published as a conference paper at ICLR 2025

## F-FIDELITY: A ROBUST FRAMEWORK FOR FAITHFUL### NESS EVALUATION OF EXPLAINABLE AI


**Xu Zheng** **[1]** **, Farhad Shirani** **[1]** [ �] **, Zhuomin Chen** **[1]** **, Chaohao Lin** **[1]** **, Wei Cheng** **[2]** **,**
**Wenbo Guo** **[3]**, **Dongsheng Luo** **[1]** [ �]

1Florida International University, Miami, United States
2NEC Laboratories America, Princeton, United States
3University of California, Santa Barbara, United States
_{_ xzhen019,fshirani,zchen051,clin027,dluo _}_ @fiu.edu
weicheng@nec-labs.com
henrygwb@ucsb.edu


ABSTRACT


Recent research has developed a number of eXplainable AI (XAI) techniques,
such as gradient-based approaches, input perturbation-base methods, and blackbox explanation methods. While these XAI techniques can extract meaningful insights from deep learning models, how to properly evaluate them remains an open
problem. The most widely used approach is to perturb or even remove what the
XAI method considers to be the most important features in an input and observe
the changes in the output prediction. This approach, although straightforward,
suffers the Out-of-Distribution (OOD) problem as the perturbed samples may no
longer follow the original data distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by retraining the model with perturbed samples guided by explanations. However, using the model retrained based on XAI
methods to evaluate these explainers may cause information leakage and thus lead
to unfair comparisons. We propose Fine-tuned Fidelity (F-Fidelity), a robust evaluation framework for XAI, which utilizes i) an explanation-agnostic fine-tuning
strategy, thus mitigating the information leakage issue, and ii) a random masking
operation that ensures that the removal step does not generate an OOD input. We
also design controlled experiments with state-of-the-art (SOTA) explainers and
their degraded version to verify the correctness of our framework. We conduct
experiments on multiple data modalities, such as images, time series, and natural language. The results demonstrate that F-Fidelity significantly improves upon
prior evaluation metrics in recovering the ground-truth ranking of the explainers.
Furthermore, we show both theoretically and empirically that, given a faithful explainer, F-Fidelity metric can be used to compute the sparsity of influential input
components, i.e., to extract the true explanation size. The source code is available
[at https://trustai4s-lab.github.io/ffidelity.](https://trustai4s-lab.github.io/ffidelity)


1 INTRODUCTION


EXplainable AI (XAI) methods have been widely used in many domains, such as Computer Vision
(CV) (Chattopadhay et al., 2018; Smilkov et al., 2017; Jiang et al., 2021; Zhou et al., 2016; Selvaraju et al., 2017), Neural Language Processing (NLP) (Lyu et al., 2024; Luo et al., 2024; Zhao
et al., 2024), Graph Neural Networks (GNNs) (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2020;
Vu & Thai, 2020), and Time Series (Liu et al.; Queen et al., 2024). There are various types of explanation methods, in which the most predominant one is post-hoc instance-level explanation. Given a
pre-trained classifier and a specific input, these methods aim to identify the most important features
of the model’s output. For instance, such explanations map to a subset of important pixels in image
classification (Ribeiro et al., 2016; Selvaraju et al., 2017; Lundberg, 2017). Existing research has
proposed a number of XAI methods to draw post-hoc explanations — such as Integrated Gradi

 - Corresponding authors


1


Published as a conference paper at ICLR 2025


ents (Sundararajan et al., 2017), CAM-based approaches (Selvaraju et al., 2017), and SmoothGrad
(Smilkov et al., 2017).


Despite extracting useful insights about model decisions, how to faithfully evaluate and compare explanation methods remains an open challenge. There have been some existing efforts to address this
issue. The preliminary works create datasets with “ground-truth” explanations for XAI evaluation,
such as the MS-CoCo dataset in CV, (Lin et al., 2014), the Mutag dataset in graph (Debnath et al.,
1991), and the e-SNLI dataset in NLP (Camburu et al., 2018). However, these datasets are still limited to certain tasks and cannot be generalized. More importantly, these “ground-truth” explanations
are created based on humans’ understanding of a data sample, which may not reflect an ML model’s
decision-making processes, and thus may still give unfaithful evaluations. More recent works propose the _removal strategy_, which does not rely on “ground-truth” explanations (Zhou et al., 2016;
Selvaraju et al., 2017; Rong et al., 2022; Hooker et al., 2019; Madsen et al., 2023; 2022; Hase et al.,
2021; Zheng et al., 2024; Yuan et al., 2022). Technically speaking, the removal strategy removes
certain parts of an input that are deemed as (non-)important and records the changes in the model
prediction. A larger drop in accuracy when removing the important parts reflects the removed parts
as indeed important and thus validates the faithfulness of the corresponding explanation method.
For example, in computer vision, removal means setting important pixels to zero/black; in natural
language processing, it means replacing chosen tokens with “[MASK]” token or zero embeddings.
Specific metrics designed based on the removal strategy include the Most Relevant First (MoRF)
and Least Relevant First (LeRF) (Samek et al., 2016; Yuan et al., 2022) in CV and Importance Measures (Madsen et al., 2021; 2023) in NLP. However, these removal-based explanations suffer from
the Out-Of-Distribution (OOD) issue (Hooker et al., 2019), as the perturbed samples with removed
features may no longer follow the original distribution. As a result, the model predictions of the
perturbed samples are unreliable, regardless of the explanation faithfulness.


To address the OOD problem, RemOve And Retrain (ROAR) (Hooker et al., 2019), proposed retraining the model for evaluating the explainer, where the model is trained using explanation-guided
removal. However, retraining the model based on the explainer output leads to an information leakage issue (Rong et al., 2022). An additional issue, which has been overlooked in prior works, is
that the removal strategy is highly dependent on explanation size ( or sparsity). That is, the size of
the explanation part that is removed affects the evaluation output. In the absence of ground-truth
explanations, it is not clear what fraction of the input elements should be removed as part of the
explanation, which leads to inconsistent evaluations.


To address the aforementioned challenges, we introduce Fine-tuned Fidelity (F-Fidelity), a novel
framework to evaluate the performance of explainers. F-Fidelity leverages i) an explanation-agnostic
fine-tuning strategy to prevent information leakage and ii) a controlled random masking operation
to overcome the OOD issues observed in the application of prior evaluation metrics. The finetuning process employs stochastic masking operations, such as randomly dropping pixels in images,
tokens in language, or time steps in time series data, to generate augmented training samples. This
augmented data is then used to fine-tune a surrogate model. Unlike previous approaches such as
ROAR (Hooker et al., 2019), our strategy effectively mitigates the risk of information leakage and
label bias by using explanation-agnostic stochastic masks, while also offering improved efficiency
as it eliminates the need to retrain the model for each individual explainer. During the evaluation
phase, F-Fidelity implements a removal strategy that generates stochastic masks conditioned on the
explainer output, designed to be in-distribution with respect to the masks used in the fine-tuning
step, which mitigates the OOD issues observed in the application of prior evaluation metrics.


To verify the effectiveness of F-Fidelity, we provide comprehensive empirical faithfulness evaluations on a collection of explainers that are systematically degraded from an original explainer
through controlled random perturbations. Thus the correct (ground-truth) ranking of the explainers,
in terms of faithfulness, is known beforehand. The experiments on multiple data modalities demonstrate the robustness of F-Fidelity through both macro and micro correlations. Macro correlations
measure the Spearman rank correlation between the ground truth and the overall performance across
all sparsity levels, while micro correlations capture the average Spearman rank correlation between
the ground truth and performance at individual sparsity levels. In both aspects, F-Fidelity consistently outperforms existing methods. Furthermore, we show both theoretically and empirically that,
given a faithful explainer, the F-Fidelity metric can be used to compute the size (or sparsity) of
ground-truth explanations. To elaborate, we show that the F-Fidelity metric output, when evaluated


2


Published as a conference paper at ICLR 2025


a function of mask size, produces a piecewise constant function, where the length of the constant
pieces depends on the explanation size. The following summarizes our main contributions:


- We introduce a novel evaluation framework for measuring explanation faithfulness with strong
theoretical principles that is robust to distribution shifts.


- We design and implement a rigorous experimental setting to fairly compare metrics. Our comprehensive evaluations across multiple data modalities (images, time series, and natural language)
demonstrate the superior performance and broad applicability of F-Fidelity.


- We theoretically and empirically analyze the relationship between the explanation size and the FFidelity metrics and demonstrate in F-Fidelity, the ground truth explanation sizes can be inferred.


2 PRELIMINARIES


2.1 NOTATION


Sets are denoted by calligraphic letters such as _X_ . The set _{_ 1 _,_ 2 _, · · ·, n}_ is represented by [ _n_ ].
Multidimensional arrays (tensors) are denoted by bold-face letters such as **x** . Upper-case letters
such as _X_ represent random variables, and lower-case letters such as _x_ represent their realizations.
Similarly, random tensors are denoted by upper-case letters such as **X** .


2.2 CLASSIFICATION MODELS AND EXPLANATION FUNCTIONS


Let _f_ : **X** _�→_ _Y_ be a (pre-trained) classification model — such as a neural network — which
takes an input **X** _∈_ R _[t][×][d]_ and outputs a label _Y ∈Y_, where _Y_ is a finite set of labels. In CV
tasks, _t_ = _h × w_, where _h, w_ are the height and width, and _d_ is the number of channels per input
pixel. Analogously, in NLP and time series classification tasks, _t ∈_ N represents the time index,
and _d_ is the feature dimension. An explanation function (explainer) consists of a pair of mappings
_ψ_ = ( _ϕ, ξ_ ), where _ϕ_ : **X** _�→_ R _[t]_ + _[×][d]_ is the score function, mapping each input element to its (nonnegative) important score, and a mask function _ξ_ : _ϕ_ ( **X** ) _�→_ **M**, mapping the output of the score
function to a binary mask **M** _∈{_ 0 _,_ 1 _}_ _[t][×][d]_ . The masked input **X** _⊙_ **M** is called the explanation for the
input **X** and model _f_ ( _·_ ), where _⊙_ represents elementwise multiplication. We denote the explanation
size by _S_ = _∥_ **M** _∥_ 1, where _∥· ∥_ 1 is the _ℓ_ 1 norm. That is, the explanation size _S_ is the number of
non-zero elements of **M** . In general, the size may be deterministically set to a constant value _s_,
or alternatively, it may depend on the output of the score function, e.g., input elements receiving a
score higher than a given threshold are included in the mask and the rest are removed. Let us assume
that the (ground-truth) data distribution is _P_ **X** _,Y_ . Then, a ‘good’ explainer is one which minimizes
the total variation distance _dT V_ ( _PY |_ **X** _, PY |_ **X** _⊙_ **M** ), while satisfying an explanation size constraint
E **X** ( _∥_ **M** _∥_ 1) _≤_ _s_, where _s ∈_ N is the desired average explanation size. The minimization of the
total variation essentially enforces that the posterior distribution of the classifier output be mostly
determined by the masked input explanation, implying that the subset of input components which
are removed by the mask have a low influence on the classifier output.


2.3 QUANTIFYING THE PERFORMANCE OF EXPLAINERS


A key challenge in explainability research is to quantify and compare the performance of various
explainers. The performance of an explainer can be formally quantified in terms the total variation
distance _dT V_ ( _PY |_ **X** _, PY |_ **X** _⊙_ **M** ) as a function of the average explanation size E **X** ( _∥_ **M** _∥_ 1). However,
in most problems of interest, the underlying statistics _P_ **X** _,Y_ is not available, and hence direct evaluation of the aforementioned total variation distance is not possible. As discussed in the introduction,
some datasets are accompanied by ground-truth explanations, which enables the use of measures
such as AUC and IoU for evaluating the quality explainers. However, the ground-truth explanations
are available only for a limited collection of datasets, and even when ground-truth explanations are
available, they may not accurately reflect the model’s internal decision-making processes. To address the aforementioned issues, a widely used set of metrics have been proposed in the literature
which are based on the removal strategy. In CV, two removal orders have been considered (Samek
et al., 2016; Yuan et al., 2022): MoRF, which evaluates explanations by removing the most influential pixels first, and LeRF, which begins with removing the least influential pixels. These approaches


3


Published as a conference paper at ICLR 2025


provide complementary perspectives on feature importance - MoRF assesses whether removing important features significantly impacts predictions, while LeRF verifies if retaining important features
is sufficient for model performance. In the graph domain, an alternative but conceptually related removal strategy is used, where first the size of explanation subgraphs is determined according to a
sparsity parameter, and then edges are removed either from the explanation subgraph of the desired
size, or the non-explanation subgraph which is its complement. In this paper, we use metrics based
on this graph domain removal strategy, called the _Fidelity_ (Fid) metric in the literature (Pope et al.,
2019; Bajaj et al., 2021; Yuan et al., 2022). The Fidelity metrics align with MoRF and LeRF principles through _Fid_ [+] and _Fid_ _[−]_ respectively. Formally, given the input and label pair ( **x** _, y_ ) and a
binary mask **m**, the _Fidelity_ metrics are defined as follows:



1
_Fid_ [+] ( _ψ_ ) =
_|T |_


1
_Fid_ _[−]_ ( _ψ_ ) =
_|T |_




 - 1( _yi_ = _f_ ( **x** _i_ )) _−_ 1( _yi_ = _f_ ( **x** _i −_ **x** _i ⊙_ **m** _i_ )) (1)


( **x** _i,yi_ ) _∈T_


 - 1( _yi_ = _f_ ( **x** _i_ )) _−_ 1( _yi_ = _f_ ( **x** _i ⊙_ **m** _i_ )) _,_ (2)


( **x** _i,yi_ ) _∈T_











where _T_ is the dataset used for evaluating the performance of the explainer, _n_ is the size of the
dataset, 1( _·_ ) denotes the indicator function, and **m** _i_ = _ψ_ ( **x** _i_ ) is the explanation corresponding to **x** _i_
produced by the explainer _ψ_ ( _·_ ). Here, _Fid_ [+] measures prediction changes when removing important
features (similar to MoRF), while _Fid_ _[−]_ evaluates model performance when keeping only important
features (similar to LeRF).


A significant limitation of explanation evaluation through removal strategies is the OOD problem (Hooker et al., 2019; Zheng et al., 2024). When we remove elements from an input - whether
they are pixels in images, time steps in time series, or edges in graphs - the modified input may no
longer follow the original data distribution that the model was trained on. For instance, when evaluating image explanations by zeroing out important pixels, the resulting images with black patches
are unlikely to resemble natural images. Consequently, the model’s predictions on these modified
inputs may be unreliable, not due to low quality of the explanation itself, but because the model is
operating outside its training distribution.


This OOD problem has been analyzed through different approaches. In ROAR (Hooker et al., 2019),
which focused on CV tasks, the solution was to retrain the model on perturbed data where important
input elements identified by the explainer are removed. While this approach mitigates the OOD
issue, Rong et al. (2022) identified that it suffers from information leakage through the binary masks
of removed pixels - the pattern of removals itself can contain enough class information to affect the
evaluation outcome. Moreover, ROAR requires retraining a separate model for each explainer being evaluated, making it computationally expensive and time-consuming when comparing multiple
explanation methods.


Similarly, In R-Fidelity (Zheng et al., 2024), which considered the evaluation of GNN explainers,
it was argued that the Fidelity metric highly relies on the robustness of the underlying classifier to
removal of potentially large sections of the input, e.g.,the removal of a large subgraph explanation
for _Fid_ [+] or its complement for _Fid_ _[−]_ . That is, the classifier should be robust to OOD inputs for
the Fidelity metric to align with those of the (theoretically justified) total-variation-based metric discussed in the previous sections. Rather than retraining the model, R-Fidelity introduces a stochastic
removal strategy that addresses the OOD issue by controlling the size of removed sections and randomly sampling which elements to remove, thus limiting the distribution shift of perturbed inputs.
Specifically, the following _Robust Fidelity metrics_ (RFid) was introduced:



1
_RFid_ [+] ( _ψ, α_ [+] _, s_ ) =
_|T |_


1
_RFid_ _[−]_ ( _ψ, α_ _[−]_ _, s_ ) =
_|T |_




 - 1( _yi_ = _f_ ( **x** _i_ )) _−_ _P_ ( _yi_ = _f_ ( _χ_ [+] ( **x** _i, α_ [+] _, s_ )) (3)


( **x** _i,yi_ ) _∈T_


 - 1( _yi_ = _f_ ( **x** _i_ )) _−_ _P_ ( _yi_ = _f_ ( _χ_ _[−]_ ( **x** _i, α_ _[−]_ _, s_ )) _,_ (4)


( **x** _i,yi_ ) _∈T_











where _χ_ [+] ( **x** _i, α_ [+] _, s_ ) is a sampling function which randomly, uniformly, and independently removes
_⌊sα_ [+] _⌋_ elements from the _s_ highest scoring elements of **x** _i_ based on the scores produced by _ϕ_ ( **x** _i_ ),
and _χ_ _[−]_ ( **x** _i, α_ _[−]_ ) removes _⌈_ ( _td −_ _s_ ) _α_ _[−]_ _⌉_ elements from the lowest scoring _td −_ _s_ elements. If _α_ [+] =
_α_ _[−]_ = 1, then the RFid metric reduces to the Fid metric. On the other hand, as _α_ [+] and _α_ _[−]_ are


4


Published as a conference paper at ICLR 2025


decreased, fewer input elements are removed, hence requiring lower OOD robustness to ensure the
accuracy of the evaluation output.


3 ROBUST FIDELITY VIA FINE-TUNING AND STOCHASTIC REMOVAL


As discussed in the previous section, a significant limitation of prior explanation evaluation metrics
is the loss in accuracy due to the OOD nature of the modified inputs generated by the application of
removal strategies. For instance, the probability difference _P_ ( _Y_ = _f_ ( **X** )) _−P_ ( _Y_ = _f_ ( **X** _−_ **X** _⊙_ **M** ))
may be large, even for low-quality explanations. This occurs because the modified input **X** _−_ **X** _⊙_ **M**
is OOD for the trained classifier _f_ ( _·_ ), despite _PY |_ **X** and _PY |_ **X** _−_ **X** _⊙_ **M** being close to each other.
Consequently, this yields a high _Fid_ [+] score despite the explanation’s low quality with respect to
the theoretically justified total variation metric. For example, in the empirical evaluations provided
in the next sections, we demonstrate that the Fid measure sometimes assigns better evaluations to
completely random explainers than to those whose outputs align with ground-truth explanations.


A partial solution in the graph domain addresses this issue by removing only an _α_ [+] fraction of
the explanation subgraph and _α_ _[−]_ fraction of the non-explanation subgraphs (Zheng et al., 2024).
However, we argue that two issues degrade the evaluation quality of the RFid metric. First, the
classifier may lack robustness and produce unreliable outputs even when the input is only slightly
perturbed. Second, if the original explanation size is large (small), then removing an _α_ [+] ( _α_ _[−]_ ) of the
explanation (non-explanation) part of the input, this would still yield OOD inputs.


In this work, we introduce a simple yet effective framework, F-Fidelity, for robust evaluation of XAI
methods. Our strategy can be summarized as **fine-tuning and stochastic removal** . Specifically,
we first fine-tune the model with randomly masked inputs to improve its robustness to perturbation.
Then employ a controlled stochastic removal process that ensures the perturbed inputs remain within
the distribution seen during fine-tuning.


To achieve reliable predictions on partially removed inputs, we design a fine-tuning process that
randomly removes up to _β ∈_ [0 _,_ 1] ratio of input elements. To elaborate, we introduce a stochastic
mask generator _Pβ_ : ( _t, d_ ) _�→_ **M** _β_, which takes the input dimensions ( _t, d_ ) as input and outputs a
mask **M** _β ∈{_ 0 _,_ 1 _}_ _[t][×][d]_ of _size βtd_, i.e. with up to _βtd_ non-zero elements. For instance, in image
classification, the mask generator is designed to select random image pixels or patches for removal.
Formally, we define the fine-tuning loss as:


_L_ = E **X** _,Y_ [ _L_ ( _f_ ( **X** _−_ _Pβ ⊙_ **X** ) _, Y_ )] _,_ (5)


where _L_ is the loss function used during training (e.g., cross-entropy).


In the evaluation process, we modify the RFid metric to ensure consistency with our fine-tuning
strategy by upper-bounding the total number of removed elements by _βtd_ - the same bound used
during fine-tuning. That is, for a fixed _β_, and RFid parameters _αorig_ [+] _[, α]_ _orig_ _[−]_ _[∈]_ [[0] _[,]_ [ 1]][, we set the]
upper-bounded RFid parameters as


_βtd_

_α_ [+] = min( _αorig_ [+] _[, ][βtd]_ and _α_ _[−]_ = min( _αorig_ _[−]_ _[,]_ (6)

_s_ [)] ( _td −_ _s_ ) [)] _[,]_


so that the sampling functions _χ_ [+] and _χ_ _[−]_ remove the minimum of _αorig_ [+] _[s]_ [ (based on explanation]
size) and _βtd_ (based on input size) elements for _χ_ [+], and the minimum of _αorig_ _[−]_ [(] _[td][ −]_ _[s]_ [)][ and] _[ βtd]_
elements for _χ_ _[−]_, providing absolute upper bounds on the number of removed elements.


The pipeline of our method is shown in Algorithm 1. We denote the resulting metrics, which use
the fine-tuning process and the _RFid_ [+] and _RFid_ _[−]_ metrics with sampling rates that are truncated
based on _β_ ( equation 6), as _FFid_ [+] ( _ψ, αorig_ [+] _[, β, s]_ [)][ and] _[ FFid][−]_ [(] _[ψ, α]_ _orig_ _[−]_ _[, β, s]_ [)][, respectively.]


It is worth noting that both _FFid_ [+] and _FFid_ _[−]_ can take negative values in certain cases. This occurs
when the accuracy after masking exceeds the original prediction accuracy. Alternative formulations
could enforce positive values by only reporting masked accuracy like ROAR (Hooker et al., 2019)
and deletion/insertion scores (Petsiuk et al., 2018; Pan et al., 2021).


5


Published as a conference paper at ICLR 2025


4 DETERMINING THE EXPLANATION SIZE VIA FIDELITY METRICS


A critical challenge in explainable AI is determining the appropriate size or scope of explanations.
Ideally, ground truth explanations would be discretized into distinct clusters representing different
levels of importance. For instance, in image classification, pixels associated with the target object
tend to receive high importance scores. Conversely, pixels corresponding to the background or
irrelevant regions receive low scores. However, in many practical scenarios, even _good_ explainers
that produce accurate explanation masks — as measured by the Fid and RFid evaluation metrics —
may yield explanation scores that are not discretized into distinct clusters.


In this section, we theoretically demonstrate that our proposed evaluation metric can recover the
cluster sizes given an explainer that outputs the correct explanation mask (i.e., correctly ranks the
importance of input elements). Thus, provided the explainer outputs an accurate mask function, our
metric can recover the explanation size (also known as sparsity). To provide a concrete theoretical
analysis, we first consider a classification problem under a set of idealized assumptions that generalize the above observations on the clustering of explanation scores. Specifically, let us consider a
classification task defined by a joint distribution _P_ **X** _,Y_ and a classifier _f_ : **x** _�→_ _y_ . We assume that
the input elements can be partitioned into several _influence tiers_ . That is, for any given input **x**, there
exists a partition _Ck_ ( **x** ) _, k ∈_ [ _r_ ] of the index set [ _t_ ] _×_ [ _d_ ], where _Ck_ ( **x** ) represents the set of indices
of the input elements belonging to tier _k_, and _ck_ = _|Ck_ ( **x** ) _|_ are the (fixed) tier sizes. For a given
mask **m**, the probability of correct classification based on the masked input **x** _⊙_ **m** depends only on
the counts of unmasked elements in each influence tier. Formally, _P_ ( _Y |_ **x** _⊙_ **m** ) = _g_ ( _j_ 1 _, j_ 2 _, . . ., jr_ ),
where _g_ : [ _c_ 1] _×_ [ _c_ 2] _× · · · ×_ [ _cr_ ] _→_ [0 _,_ 1] is a function monotonically increasing with respect to the
lexicographic ordering on its input, and _jk ∈_ [ _ck_ ] is the number of elements in _Ck_ ( **x** ) whose corresponding mask element in **m** is non-zero (unmasked). We further focus on Shapley-value-based
explanations, which provide a theoretical foundation for our analysis. Recall that, given label _y_, the
Shapley value associated with an element ( _i, j_ ) _∈_ [ _t_ ] _×_ [ _d_ ] of **x** is given as (Lundberg, 2017):



_S_ **x** ( _i, j_ ) = 

**m** : _mi,j_ =0



_∥m∥_ 1!( _td −∥m∥_ 1 _−_ 1)!

( _P_ ( _Y_ = _y|_ **x** _⊙_ **m** _[′]_ ) _−_ _P_ ( _Y_ = _y|_ **x** _⊙_ **m** )) _,_
( _td_ )!



where **m** _[′]_ is the mask obtained from **m** by setting _m_ _[′]_ _i,j_ [= 1][ (unmasking the][ (] _[i, j]_ [)][ element). Under]
the aforementioned influence tier assumption, it is straightforward to verify that input elements
within the same influence tier receive equal Shapley values. Specifically, for any _k ∈_ [ _r_ ] and any
( _i, j_ ) _,_ ( _i_ _[′]_ _, j_ _[′]_ ) _∈Ck_ ( **x** ), we have _S_ **x** ( _i, j_ ) = _S_ **x** ( _i_ _[′]_ _, j_ _[′]_ ).
**Theorem 1.** _For the classification task described above, and a given pre-trained classifier f_ ( _·_ ) _,_
_consider a Shapley-value-based explainer ψ_ ( _·_ ) _. For αorig_ [+] _[∈]_ [[0] _[,]_ [ 1]] _[ and][ β][ ∈]_ [[0] _[, α]_ [+][]] _[, let]_

_e_ ( _s_ ) = E **X** _,Y_ ( _FFid_ [+] ( _ψ, αorig_ [+] _[, β, s]_ [))] _[,]_ _s ∈_ [0 _, td_ ] _._


_Then, e_ ( _s_ ) _is monotonically increasing for s ∈_ [0 _, c_ 1] _and monotonically decreasing for s ∈_

[max( _β_
_α_ [+] _orig_ _[td, c]_ [1][)] _[, td]_ []] _[.]_


The proof is provided in the Appendix.


This theorem shows that _FFid_ [+] can recover the size of the most influential tier (i.e., the first cluster
size) when the explainer’s ranking is close to that of an ideal Shapley-based explainer. Specifically,
the value of _s_ in which _FFid_ [+] changes direction corresponds to the size of an influence tier. This is
verified in the empirical evaluation provided in Section D.8. This result implies that even when an
explainer provides continuous scores without distinct clustering, our metric can infer the underlying
discrete structure of the ground truth explanations and recover the explanation size.


5 EXPERIMENTS


To demonstrate the robustness of the F-Fidelity framework, we conduct comprehensive experiments
across multiple domains, including image classification, time series analysis, and natural language
processing. Our evaluation strategy builds upon the concept introduced by Rong et al. (2022), which
posits that an ideal evaluation method should yield consistent rankings in both MoRF and LeRF settings. To further establish a controlled experimental setting with ground truth (GT) rankings, we


6


Published as a conference paper at ICLR 2025


introduce a novel approach for a fair comparison using a degradation operation on a good explanation, such as an explanation obtained by Integrated Gradients (IG) (Sundararajan et al., 2017),
generating a series of explanations with varying levels of random noise. Specifically, we first obtain
initial (good) explanations using well-established explainers, then systematically degrade them by
adding different ratios of random noise. Since explanation quality naturally decreases with increased
noise, this creates a ground truth ranking where explanations with less noise should rank higher.


We evaluate the performance of F-Fidelity against established baselines such as (Fidelity),
ROAR (Hooker et al., 2019), and R-Fidelity (Zheng et al., 2024) across a wide range of sparsity
levels, from 5% to 95% at 5% intervals. Throughout our evaluation, we focus on three key Spearman rank correlations that measure how well different evaluation metrics align with ground truth and
each other. Specifically, “MoRF vs. GT” measures how well _Fid_ [+] rankings align with ground truth
rankings when removing important features first, while “LeRF vs. GT” measures the correlation between _Fid_ _[−]_ rankings and ground truth when retaining important features. The “MoRF vs. LeRF”
correlation assesses the consistency between _Fid_ [+] and _Fid_ _[−]_ evaluations, where strong negative
correlation indicates that features identified as important by one metric are consistently identified as
important by the other. These correlations allow us to assess the methods’ performance under various conditions, from highly sparse to nearly complete explanations. To provide a thorough analysis,
we employ both macro and micro correlation metrics:


- **Macro Correlation** : Following Rong et al. (2022)’s approach of evaluating overall explainer
consistency, and inspired by the AUC-based aggregation methods in Zhu et al. (2024) and Pan
et al. (2021), we compute the AUC with respect to sparsity across the entire 5-95% range for
each explanation method. The macro correlations are then calculated using these AUC values,
providing an overall performance measure across all sparsity levels.

- **Micro Correlation** : To capture fine-grained performance differences, we calculate micro correlations at each sparsity level. In the main body of the paper, we report the averaged micro
correlations across all sparsity levels, as well as the average rank of each method.


5.1 IMAGE CLASSIFICATION EXPLANATION EVALUATION


**Setup** . We use CIFAR-100 (Krizhevsky et al., 2009) and Tiny-Imagenet (Deng et al., 2009) [1], as
the benchmark datasets. To obtain a pre-trained model to be explained, we adopt ResNet (He
et al., 2016). More experiments with Vision Transformer(ViT) (Dosovitskiy et al., 2020) can be
found in Appendix D.1. To generate different explanations, we first use two explanation methods, SmoothGrad Squared (SG-SQ) (Smilkov et al., 2017) and GradCAM (Selvaraju et al., 2017)
to obtain the explanations. Then we use a set proportion of noise perturbations in the image,

[0 _._ 0 _,_ 0 _._ 2 _,_ 0 _._ 4 _,_ 0 _._ 6 _,_ 0 _._ 8 _,_ 1 _._ 0]. We provide the implementation detail in Appendix C.


**Results** . As shown in Table 1 and Table 2, F-Fidelity achieves superior performances across different
explainers and correlation metrics. Traditional Fidelity and ROAR methods show inconsistent and
often poor performance, particularly in MoRF scenarios. Fidelity suffers from the out-of-distribution
(OOD) issue, leading to unreliable evaluations when features are removed. ROAR, while addressing
the OOD problem through retraining, faces challenges with information leakage and potential convergence issues, resulting in suboptimal correlations. In contrast, for both SG-SQ and GradCAM,
F-Fidelity consistently achieves optimal or near-optimal performance in ranking explanations. In
CIFAR-100, F-Fidelity achieves perfect Macro and Micro correlations for all three cases with SGSQ. For GradCAM, it shows strong negative correlation (-0.60 to -0.71) in MoRF comparisons,
significantly outperforming other methods. Tiny ImageNet results further reinforce its effectiveness
with perfect correlations (-1.00) across all metrics for both explainers. Notably, F-Fidelity consistently ranks first in the micro rank evaluation for “MoRF vs. GT” and “MoRF vs. LeRF” correlations
across both datasets and explainers, indicating its robust performance across various sparsity levels.


5.2 TIME SERIES CLASSIFICATION EXPLANATION EVALUATION


**Setup.** We use two benchmark datasets for time series analysis: PAM for human activity recognition
and Boiler for mechanical fault detection (Queen, 2023). For PAM, we use 534 samples across 8


1https://github.com/rmccorm4/Tiny-Imagenet-200?tab=readme-ov-file


7


Published as a conference paper at ICLR 2025


Table 1: Spearman rank correlation and rank results on CIFAR-100 dataset with ResNet.

|Correlation SG-SQ|GradCam|
|---|---|
|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.68_±_0.08<br>-0.66_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.68_±_0.08<br>-0.66_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|0.81_±_0.03<br>0.99_±_0.02<br>0.20_±_0.11<br>**-0.60**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>0.81_±_0.03<br>0.99_±_0.02<br>0.20_±_0.11<br>**-0.60**_±_0.00|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.36_±_0.37<br>-0.54_±_0.35<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>0.98_±_0.04<br>0.99_±_0.04<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.23_±_0.35<br>-0.53_±_0.34<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00|0.57_±_0.14<br>0.76_±_0.07<br>0.09_±_0.12<br>**-0.71**_±_0.02<br>0.99_±_0.01<br>0.99_±_0.01<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>0.57_±_0.14<br>0.75_±_0.07<br>0.01_±_0.12<br>**-1.00**_±_0.02|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>3.68_±_0.46<br>3.26_±_0.44<br>1.11_±_0.31<br>**1.00**_±_0.00<br>LeRF vs GT_ ↓_<br>**1.11**_±_0.31<br>1.21_±_0.41<br>1.42_±_0.67<br>1.42_±_0.67<br>MoRF vs LeRF_ ↓_<br>3.68_±_0460<br>3.32_±_0.46<br>1.11_±_0.31<br>**1.00**_±_0.00|3.00_±_0.46<br>3.74_±_0.55<br>2.11_±_0.79<br>**1.00**_±_0.00<br>2.05_±_1.39<br>1.68_±_1.08<br>1.21_±_0.69<br>**1.11**_±_0.45<br>3.00_±_0.45<br>3.74_±_0.55<br>2.16_±_0.74<br>**1.00**_±_0.00|



Table 2: Spearman rank correlation results on Tiny-Imagenet dataset with ResNet.

|Correlation SG-SQ|GradCam|
|---|---|
|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.69_±_0.07<br>-0.83_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.69_±_0.07<br>-0.83_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|-0.97_±_0.03<br>0.63_±_0.03<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.03<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>-0.97_±_0.03<br>0.63_±_0.03<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.38_±_0.48<br>-0.50_±_0.37<br>-0.99_±_0.03<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.01<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.38_±_0.48<br>-0.50_±_0.36<br>-0.99_±_0.03<br>**-1.00**_±_0.01|-0.42_±_0.14<br>0.54_±_0.14<br>-0.99_±_0.01<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>0.99_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>-0.42_±_0.14<br>0.55_±_0.16<br>-0.99_±_0.01<br>**-1.00**_±_0.00|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>3.74_±_0.44<br>3.21_±_0.41<br>1.16_±_0.36<br>**1.00**_±_0.00<br>LeRF vs GT_ ↓_<br>1.11_±_0.31<br>**1.00**_±_0.00<br>1.16_±_0.49<br>1.16_±_0.49<br>MoRF vs LeRF_↓_<br>3.74_±_0.44<br>3.21_±_0.41<br>1.10_±_0.31<br>**1.00**_±_0.00|3.05_±_0.51<br>3.84_±_0.36<br>1.21_±_0.41<br>**1.00**_±_0.00<br>1.47_±_0.94<br>1.58_±_1.04<br>1.16_±_0.67<br>**1.11**_±_0.44<br>3.16_±_0.36<br>3.84_±_0.36<br>1.21_±_0.41<br>**1.00**_±_0.00|



activity classes, with each sample recorded using a fixed segment window length of 600 from 17
sensors. The Boiler dataset, used for mechanical fault detection, consists of 400 samples with 20
dimensions and a fixed segment window length of 36. We employ IG from the Captum library [2]
to obtain initial explanations. To generate different explanations, we apply noise perturbations to
the importance of each timestamp, using proportions of [0 _._ 0 _,_ 0 _._ 1 _,_ 0 _._ 2 _,_ 0 _._ 3 _,_ 0 _._ 4 _,_ 0 _._ 5] for PAM and

[0 _._ 0 _,_ 0 _._ 2 _,_ 0 _._ 4 _,_ 0 _._ 6 _,_ 0 _._ 8 _,_ 1 _._ 0] for Boiler.


**Results.** Table 3 demonstrates the robust performance of F-Fidelity across different evaluation metrics. Specifically, in the PAM dataset, F-Fidelity and R-Fidelity outperform Fidelity and ROAR in
micro correlations and ranks, with F-Fidelity slightly edging out R-Fidelity in “LeRF vs GT” and
“MoRF vs LeRF” micro ranks. The Boiler dataset results reveal more significant differences among
the methods. F-Fidelity substantially outperforms other methods in “LeRF vs GT” and “MoRF vs
LeRF” macro correlations. In micro correlations and ranks for the Boiler dataset, F-Fidelity consistently achieves the best or near-best performance across all metrics. These results underscore the
effectiveness and stability of F-Fidelity in evaluating explanations for time series data.


Table 3: Spearman ranks correlations and ranks on time series datasets with LSTM as classifier. The
best performance is marked as bold. the “-” means the correlation can’t be obtained because of the
same rank of different explanations.

|Correlation PAM|Boiler|
|---|---|
|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|-0.98_±_0.03<br>-0.99_±_0.20<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>0.22_±_0.03<br>0.36_±_0.11<br>0.53_±_0.06<br>**0.86**_±_0.07<br>-0.27_±_0.08<br>-0.38_±_0.14<br>-0.53_±_0.06<br>**-0.86**_±_0.07|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.88_±_0.39<br>-0.97_±_0.09<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>0.74_±_0.33<br>0.80_±_0.21<br>**1.00**_±_0.01<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.65_±_0.45<br>-0.77_±_0.21<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00|-<br>-<br>**-0.79**_±_0.31<br>-0.78_±_0.29<br>-<br>-<br>0.69_±_0.35<br>**0.79**_±_0.27<br>-<br>-<br>-0.81_±_0.27<br>**-0.97**_±_0.03|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>1.53_±_0.82<br>1.37_±_0.58<br>**1.00**_±_0.00<br>1.05_±_0.22<br>LeRF vs GT_ ↓_<br>2.32_±_1.09<br>2.37_±_0.74<br>1.16_±_0.36<br>**1.05**_±_0.22<br>MoRF vs LeRF_ ↓_<br>2.37_±_1.09<br>2.42_±_0.82<br>1.16_±_0.36<br>**1.11**_±_0.31|2.95_±_0.51<br>2.73_±_0.71<br>**1.21**_±_0.52<br>1.52_±_0.50<br>2.59_±_0.99<br>2.74_±_0.64<br>2.21_±_1.00<br>**1.68**_±_0.86<br>2.95_±_0.22<br>2.95_±_0.22<br>1.89_±_0.31<br>**1.05**_±_0.22|



2https://github.com/pytorch/captum


8


Published as a conference paper at ICLR 2025


5.3 NATURAL LANGUAGE CLASSIFICATION EXPLANATION EVALUATION


**Setup.** We use two benchmark datasets for our NLP experiments: the Stanford Sentiment Treebank (SST2) (Socher et al., 2013) for binary sentiment classification and the Boolean Questions
(BoolQ) (Socher et al., 2013) dataset for question-answering tasks. For SST2, we utilize 67,349
sentences for training and 872 for testing. BoolQ comprises 9,427 question-answer pairs for training and 3,270 for testing. We employ two popular model architectures: LSTM networks and
Transformer-based models (Appendix D.1). We follow the setting in image classification. Specifically, to generate explanations, we first use IG to obtain initial explanations, then apply noise perturbations to the importance of each timestamp at levels of [0 _._ 0 _,_ 0 _._ 2 _,_ 0 _._ 4 _,_ 0 _._ 6 _,_ 0 _._ 8 _,_ 1 _._ 0]. We compare
our F-Fidelity method against baselines including Fidelity and R-Fidelity, evaluating performance
using both macro and micro Spearman correlations. We use the Adam optimizer (Kingma & Ba,
2015) with a learning rate of 1e-4, keeping other hyperparameters at their default values.


Table 4: Spearman rank correlation and rank results with LSTM model on SST2 and BoolQ datasets.

|Correlation SST2|BoolQ|
|---|---|
|Fidelity<br>R-Fidelity<br>F-Fidelity|Fidelity<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>**-1.00**_±_0.01<br>**-1.00**_±_0.01<br>-0.99_±_0.03<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>1.00_±_0.01<br>MoRF vs LeRF_ ↓_<br>**-1.00**_±_0.01<br>**-1.00**_±_0.01<br>-0.99_±_0.02|**-1.00**_±_0.01<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**-1.00**_±_0.01<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>1.05_±_0.22<br>LeRF vs GT_ ↓_<br>**1.05**_±_0.22<br>**1.05**_±_0.22<br>**1.05**_±_0.22<br>MoRF vs LeRF_ ↓_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>1.11_±_0.31|**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00|



**Results.** The results are shown in Table 3. Our experiments in the NLP domain reveal an interesting phenomenon that NLP models demonstrate remarkable robustness to OOD inputs, which
is reflected in the performance of various fidelity metrics. Notably, the vanilla Fidelity (Fidelity)
method achieves excellent results, often matching or closely approaching the performance of more
sophisticated methods like R-Fidelity and our proposed F-Fidelity. This is evident from the nearperfect correlations (-1.00 or 1.00) across most metrics in both datasets. This strong performance of
vanilla fidelity suggests that the OOD problem, which often necessitates more complex evaluation
frameworks in other domains, may be less pronounced in NLP tasks. The robustness of NLP models
to perturbations in input tokens likely contributes to this phenomenon, allowing simpler evaluation
methods to maintain their efficacy. However, it’s worth noting that our proposed F-Fidelity method
still demonstrates consistent top-tier performance across all metrics and models, reinforcing its versatility and reliability even in scenarios where simpler methods perform well.


5.4 PRACTICAL GUIDELINES FOR F-FIDELITY USAGE


We provide in-depth analysis on the relation between model robustness and performance of FFidelity in Appendix D.2. The observed robustness patterns exhibit a direct correlation with FFidelity’s effectiveness, which provides practical guidance. For tasks where models exhibit significant sensitivity to perturbations, such as computer vision and time series models, F-Fidelity should
be the preferred choice over traditional fidelity metrics to ensure reliable explanation evaluation. In
these domains, the substantial performance degradation under perturbation indicates vulnerability to
OOD issues, making F-Fidelity’s robustness-enhancing properties particularly valuable. However,
for tasks where models demonstrate inherent robustness to feature removal, such as many NLP tasks,
simpler fidelity metrics are preferred. This relationship between model robustness and evaluation
method choice enables one to make informed decisions based on their specific domain characteristics and accuracy requirements. In Appendix D.2, we provide a simple robustness test by measuring
model accuracy under different masking ratios to determine the most appropriate evaluation metric.


9


Published as a conference paper at ICLR 2025


6 RELATED WORK


Existing methods for evaluating explanations can be generally divided into two categories according
to whether ground truth explanations are available. Comparing to the ground truth is an intuitive way
for explanation evaluation. For example, in time series data and graph data, there exist some synthetic datasets for explanation evaluation, including BA-Shapes (Ying et al., 2019), BA-Motifs (Luo
et al., 2020), FreqShapes, SeqComb-UV, SeqComb-MV, and LowVar (Queen et al., 2024; Liu et al.).
In computer Vision and Natural Language Processing, the important parts can also be obtained with
human annotation. However, these methods suffer from the heavy labor of ground truth annotation.


The second category of evaluation methods assesses the explanation quality by comparing model
outputs between the original input and inputs modified based on the generated explanations Zheng
et al. (2024). For example, Class Activation Mapping (CAM) (Zhou et al., 2016) first compares
the classification performance between the original and their GAP networks, which makes sure the
explanation is faithful for the original network. Grad-CAM(Selvaraju et al., 2017) uses image occlusion to measure faithfulness. In the following work, Grad-CAM++ (Chattopadhay et al., 2018)
uses three metrics to evaluate the performance of explanation methods, “Average Drop %”, “% Increase in Confidence”, and “Win %”. In Adversarial Gradient Integration (Pan et al., 2021; Petsiuk
et al., 2018), the authors use “Deletion Score” and “Insertion Score” to measure the faithfulness,
where “Deletion Score” is to delete the attributions from the original input and “Insertion Score” is
to insert attributions into one blank input according to the explanations. In (Samek et al., 2016), the
LeRF/MoRF method is proposed to measure if the importance is consistent with the accuracy of the
model. However, this group of metrics does not consider the effect of the OOD issues. In (Hooker
et al., 2019), the author propose a new evaluation method ROAR to calculate the accuracy, which
avoids the OOD problem by using retrain. ROAD (Rong et al., 2022) is introduced to solve information leakage and time-consuming issues. In the graph domain, GinX-Eval extends ROAR by
introducing a fine-tuning strategy (Amara et al., 2023).


In the field of NLP, various methods are employed for evaluating faithfulness, as outlined in (Jacovi & Goldberg, 2020). These methods include axiomatic evaluation, predictive power evaluation,
robustness evaluation, and perturbation-based evaluation, among others. Axiomatic evaluation involves testing explanations based on predefined principles (Jacovi & Goldberg, 2020; Adebayo et al.,
2018; Liu et al., 2022; Wiegreffe et al., 2020). Predictive power evaluation operates on the premise
that if explanations do not lead to the corresponding predictions, they are deemed unfaithful (Jacovi & Goldberg, 2020; Sia et al., 2023; Ye et al., 2021). Robustness evaluation examines whether
explanations remain stable when there are minor changes in the input (Ju et al., 2021; Yin et al.,
2021; Zheng et al., 2021), such as when input words with similar semantics produce similar outputs.
Perturbation-based evaluation, one of the most widely used methods, assesses how explanations
change when perturbed (Atanasova, 2024; Jain & Wallace, 2019). This approach is akin to MoRF
and LeRF, where (DeYoung et al., 2019) measures prediction sufficiency and comprehensiveness
by removing both unimportant and important features. For further information, please refer to the
survey paper (Lyu et al., 2024).


7 CONCLUSION


In this paper, we introduced F-Fidelity, a robust framework for faithfulness evaluation in explainable
AI. By leveraging a novel fine-tuning process, our method significantly mitigates the OOD problem
that has plagued previous evaluation metrics. Through comprehensive experiments across multiple data modalities, we demonstrated that F-Fidelity consistently outperforms existing baselines in
assessing the quality of explanations. Notably, our framework revealed a relationship between evaluation performance and ground truth explanation size under certain conditions, providing valuable
insights into the nature of model explanations. In the future, we plan to explore alternative perturbation strategies, such as Gaussian blur. Additionally, we will investigate fine-tuning models with
limited training data to further enhance the applicability and practicality of F-Fidelity, particularly
in resource-constrained settings.


10


Published as a conference paper at ICLR 2025


ACKNOWLEDGMENTS


This project was partially supported by NSF grants IIS-2331908 and CCF-2241057. The views
and conclusions contained in this paper are those of the authors and should not be interpreted as
representing any funding agencies.


REFERENCES


Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. _Advances in neural information processing systems_, 31, 2018.


Kenza Amara, Mennatallah El-Assady, and Rex Ying. GInx-eval: Towards in-distribution evaluation
of graph neural network explanations. In _XAI in Action: Past, Present, and Future Applications_,
[2023. URL https://openreview.net/forum?id=w6Qnoy2RXG.](https://openreview.net/forum?id=w6Qnoy2RXG)


Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
_arXiv preprint arXiv:1907.02893_, 2019.


Pepa Atanasova. A diagnostic study of explainability techniques for text classification. In _Account-_
_able and Explainable Methods for Complex Reasoning over Text_, pp. 155–187. Springer, 2024.


Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong
Zhang. Robust counterfactual explanations on graph neural networks. _Advances in Neural Infor-_
_mation Processing Systems_, 34:5644–5655, 2021.


Chris M Bishop. Training with noise is equivalent to tikhonov regularization. _Neural computation_,
7(1):108–116, 1995.


Oana-Maria Camburu, Tim Rockt¨aschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural
language inference with natural language explanations. _Advances in Neural Information Process-_
_ing Systems_, 31, 2018.


Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks. In _2018_
_IEEE winter conference on applications of Computer Vision (WACV)_, pp. 839–847. IEEE, 2018.


Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal_
_chemistry_, 34(2):786–797, 1991.


Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_,
pp. 248–255. Ieee, 2009.


Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher,
and Byron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. _arXiv preprint_
_arXiv:1911.03429_, 2019.


Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Confer-_
_ence on Learning Representations_, 2020.


Peter Hase, Harry Xie, and Mohit Bansal. The out-of-distribution problem in explainability and search methods for feature importance explanations. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural_
_Information Processing Systems_, volume 34, pp. 3650–3666. Curran Associates, Inc.,
2021. [URL https://proceedings.neurips.cc/paper_files/paper/2021/](https://proceedings.neurips.cc/paper_files/paper/2021/file/1def1713ebf17722cbe300cfc1c88558-Paper.pdf)
[file/1def1713ebf17722cbe300cfc1c88558-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2021/file/1def1713ebf17722cbe300cfc1c88558-Paper.pdf)


11


Published as a conference paper at ICLR 2025


Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp.
770–778, 2016.


Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):
1735–1780, 1997.


Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability methods in deep neural networks. _Advances in neural information processing systems_, 32,
2019.


Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we
define and evaluate faithfulness? _arXiv preprint arXiv:2004.03685_, 2020.


Sarthak Jain and Byron C Wallace. Attention is not explanation. _arXiv preprint arXiv:1902.10186_,
2019.


Peng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam:
Exploring hierarchical class activation maps for localization. _IEEE Transactions on Image Pro-_
_cessing_, 30:5875–5888, 2021.


Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang, Kang Liu, and Jun Zhao. Logic traps in
evaluating attribution scores. _arXiv preprint arXiv:2109.05463_, 2021.


Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), _3rd International Conference on Learning Representations, ICLR_
_2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_ [, 2015. URL http:](http://arxiv.org/abs/1412.6980)
[//arxiv.org/abs/1412.6980.](http://arxiv.org/abs/1412.6980)


Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.


Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer_
_Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,_
_Proceedings, Part V 13_, pp. 740–755. Springer, 2014.


Yibing Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jing Li, and Shiqi Wang. Rethinking
attention-model explainability through faithfulness violation test. In _International Conference_
_on Machine Learning_, pp. 13807–13824. PMLR, 2022.


Zichuan Liu, Tianchun Wang, Jimeng Shi, Xu Zheng, Zhuomin Chen, Lei Song, Wenqian Dong,
Jayantha Obeysekera, Farhad Shirani, and Dongsheng Luo. Timex++: Learning time-series
explanations with information bottleneck. In _Forty-first International Conference on Machine_
_Learning_ .


Scott Lundberg. A unified approach to interpreting model predictions. _arXiv preprint_
_arXiv:1705.07874_, 2017.


Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang
Zhang. Parameterized explainer for graph neural network. _Advances in neural information pro-_
_cessing systems_, 33:19620–19631, 2020.


Siwen Luo, Hamish Ivison, Soyeon Caren Han, and Josiah Poon. Local interpretations for explainable natural language processing: A survey. _ACM Computing Surveys_, 56(9):1–36, 2024.


Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Towards faithful model explanation in
nlp: A survey. _Computational Linguistics_, pp. 1–67, 2024.


Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. Evaluating the faithfulness
of importance measures in nlp by recursively masking allegedly important tokens and retraining.
_arXiv preprint arXiv:2110.08412_, 2021.


12


Published as a conference paper at ICLR 2025


Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. Evaluating the faithfulness of
importance measures in nlp by recursively masking allegedly important tokens and retraining. In
_Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 1731–1751, 2022.


Andreas Madsen, Siva Reddy, and Sarath Chandar. Faithfulness measurable masked language models. _arXiv preprint arXiv:2310.07819_, 2023.


Deng Pan, Xin Li, and Dongxiao Zhu. Explaining deep neural network models with adversarial
gradient integration. In _Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)_,
2021.


Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of
black-box models. _arXiv preprint arXiv:1806.07421_, 2018.


Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In _Proceedings of the IEEE/CVF_
_conference on computer vision and pattern recognition_, pp. 10772–10781, 2019.


[Owen Queen. Replication Data for: TimeX, 2023. URL https://doi.org/10.7910/DVN/](https://doi.org/10.7910/DVN/B0DEQJ)
[B0DEQJ.](https://doi.org/10.7910/DVN/B0DEQJ)


Owen Queen, Thomas Hartvigsen, Teddy Koker, He Huan, Theodoros Tsiligkaridis, and Marinka
Zitnik. Encoding time-series explanations through self-supervised model behavior consistency.
In _Proceedings of Neural Information Processing Systems, NeurIPS_, 2023.


Owen Queen, Tom Hartvigsen, Teddy Koker, Huan He, Theodoros Tsiligkaridis, and Marinka Zitnik. Encoding time-series explanations through self-supervised model behavior consistency. _Ad-_
_vances in Neural Information Processing Systems_, 36, 2024.


Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference_
_on knowledge discovery and data mining_, pp. 1135–1144, 2016.


Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. A consistent
and efficient evaluation strategy for attribution methods. _arXiv preprint arXiv:2202.00449_, 2022.


Wojciech Samek, Alexander Binder, Gr´egoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. Evaluating the visualization of what a deep neural network has learned. _IEEE transactions_
_on neural networks and learning systems_, 28(11):2660–2673, 2016.


Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pp. 618–626,
2017.


Suzanna Sia, Anton Belyy, Amjad Almahairi, Madian Khabsa, Luke Zettlemoyer, and Lambert
Mathias. Logical satisfiability of counterfactuals for faithful explanations in nli. In _Proceedings_
_of the AAAI Conference on Artificial Intelligence_, volume 37, pp. 9837–9845, 2023.


Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. _arXiv preprint arXiv:1312.6034_, 2013.


Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. _arXiv preprint arXiv:1706.03825_, 2017.


Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In _Proceedings of the 2013 conference on empirical methods in natural language pro-_
_cessing_, pp. 1631–1642, 2013.


Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. _The journal of machine_
_learning research_, 15(1):1929–1958, 2014.


13


Published as a conference paper at ICLR 2025


Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
_International conference on machine learning_, pp. 3319–3328. PMLR, 2017.


Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph
neural networks. _Advances in neural information processing systems_, 33:12225–12235, 2020.


Sarah Wiegreffe, Ana Marasovi´c, and Noah A Smith. Measuring association between labels and
free-text rationales. _arXiv preprint arXiv:2010.12762_, 2020.


Xi Ye, Rohan Nair, and Greg Durrett. Connecting attributions and qa model behavior on realistic
counterfactuals. _arXiv preprint arXiv:2104.04515_, 2021.


Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, and Kai-Wei Chang. On the sensitivity and stability of model
interpretations in nlp. _arXiv preprint arXiv:2104.08782_, 2021.


Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. _Advances in neural information processing_
_systems_, 32, 2019.


Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. Xgnn: Towards model-level explanations of
graph neural networks. In _Proceedings of the 26th ACM SIGKDD International Conference on_
_Knowledge Discovery & Data Mining_, pp. 430–438, 2020.


Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A
taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.


Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang,
Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. _ACM Trans-_
_actions on Intelligent Systems and Technology_, 15(2):1–38, 2024.


Xu Zheng, Farhad Shirani, Tianchun Wang, Wei Cheng, Zhuomin Chen, Haifeng Chen, Hua Wei,
and Dongsheng Luo. Towards robust fidelity for evaluating explainability of graph neural networks. In _The Twelfth International Conference on Learning Representations_, 2024. URL
[https://openreview.net/forum?id=up6hr4hIQH.](https://openreview.net/forum?id=up6hr4hIQH)


Yiming Zheng, Serena Booth, Julie Shah, and Yilun Zhou. The irrationality of neural rationale
models. _arXiv preprint arXiv:2110.07550_, 2021.


Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In _Proceedings of the IEEE conference on computer_
_vision and pattern recognition_, pp. 2921–2929, 2016.


Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Jason Xue, and Jun Shen. Iterative search attribution for deep neural networks. In Ruslan Salakhutdinov, Zico Kolter,
Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.),
_Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Pro-_
_ceedings of Machine Learning Research_, pp. 62332–62348. PMLR, 21–27 Jul 2024. URL
[https://proceedings.mlr.press/v235/zhu24a.html.](https://proceedings.mlr.press/v235/zhu24a.html)


14


Published as a conference paper at ICLR 2025


A DETAILED ALGORITHM


In this section, we provided the detailed pipeline of our evaluation method F-Fidelity as follows:


**Algorithm 1** Computing _FFid_ [+], _FFid_ _[−]_



1: **Input:** model to be explained _f_, train set _Dt_, explanation test set _Dv_ with _n_ = _|Dv|_ samples, explainer _ψ_,
explanation size _s_, original parameters _αorig_ [+] [,] _[ α]_ _orig_ _[−]_ [, fixed fraction] _[ β]_ [, training epochs] _[ E]_ [, sampling numbers]
_N_
2: **Output:** _FFid_ [+], _FFid_ _[−]_



3: copy _f_ _[r]_ _←_ _f_
4: **for** _e_ in range( _E_ ) **do**
5: **for** ( **x** _i, yi_ ) in _Dt_ **do**
6: update _f_ _[r]_ by using _L_ ( _f_ ( **x** _i −_ _Pβ ⊙_ **x** _i_ ) _, yi_ )
7: **end for**
8: **end for**
9: **for** ( **x** _i, yi_ ) in _Dv_ **do**
10: **m** = _ψ_ ( **x** _i_ ) # obtain the explanation
11: **for** _k_ in range( _N_ ) **do**
12: _α_ [+] = min( _αorig_ [+] _[,]_ _[βtd]_ _s_ [)]



12: _α_ = min( _αorig_ _[,]_ _s_ [)]

13: _α_ _[−]_ = min( _αorig_ _[−]_ _[,]_ _tdβtd−s_ [)]
14: _FFid_ [+] [ _i, k_ ] _←_ 1( _yi_ = _f_ _[r]_ ( **x** _i_ )) _−_ 1( _yi_ = _f_ _[r]_ ( _χ_ [+] ( **x** _i, α_ [+] _, s_ )))
15: _FFid_ _[−]_ [ _i, k_ ] _←_ 1( _yi_ = _f_ _[r]_ ( **x** _i_ )) _−_ 1( _yi_ = _f_ _[r]_ ( _χ_ _[−]_ ( **x** _i, α_ _[−]_ _, s_ )))
16: **end for**
17: _FFid_ [+] [ _i_ ] _←_ _N_ 1 - _Nk_ =1 _[FFid]_ [+][[] _[i, k]_ []]
18: _FFid_ _[−]_ [ _i_ ] _←_ _N_ 1 - _Nk_ =1 _[FFid][−]_ [[] _[i, k]_ []]
19: **end for**
20: _FFid_ [+] _←_ _|D_ 1 _v_ _|_ �( **x** _i,yi_ ) _∈Dv_ _[FFid]_ [+][[] _[i]_ []]



20: _FFid_ [+] _←_ _|D_ 1 _v_ _|_ �( **x** _i,yi_ ) _∈Dv_ _[FFid]_ [+][[] _[i]_ []]

21: _FFid_ _[−]_ _←_ _|D_ 1 _v_ _|_ �( **x** _i,yi_ ) _∈Dv_ _[FFid][−]_ [[] _[i]_ []]



21: _FFid_ _[−]_ _←_ _|D_ 1 _v_ _|_ �( **x** _i,yi_ ) _∈Dv_ _[FFid][−]_ [[] _[i]_ []]

22: **Return** _FFid_ [+], _FFid_ _[−]_



B PROOF OF THEOREM 1


We provide the proof by considering the following cases:
**Case 1:** _s ∈_ [max( _β_
_α_ [+] _orig_ _[td, c]_ [1][)] _[, td]_ []]

We have:

_e_ ( _s_ ) = E **X** _,Y_ ( _FFid_ [+] ( _ψ, αorig_ [+] _[, β, s]_ [))]



= E **X** _,Y_ ( [1]

_n_




 - 1( _Yi_ = _fr_ ( **X** _i_ )) _−_ _P_ ( _Yi_ = _fr_ ( _χ_ [+] ( **X** _i, αorig_ [+] _[, s]_ [)))] _[,]_

( **X** _i,Yi_ ) _∈T_



where _fr_ ( _·_ ) represent the finetuned model in Algorithm 1, which is assumed to be robust to up to
_βtd_ removals, i.e., _P_ ( _fr_ ( **x** _⊙_ **m** ) = _Y_ ) = _P_ ( _Y |_ **x** _⊙_ **m** ) if _∥_ **m** _∥_ 1 _≥_ _td −_ _βtd_ . Consequently,

_e_ ( _s_ ) = _P_ **X** _,Y_ ( _Y_ = _fr_ ( **X** )) _−_ _P_ **X** _,Y_ ( _Y_ = _fr_ ( _χ_ [+] ( **X** _, αorig_ [+] _[, s]_ [)))]

= _g_ ( _c_ 1 _, c_ 2 _, · · ·, cr_ ) _−_     - _P_ ( _J_ _[r]_ = _j_ _[r]_ ) _g_ ( _c_ 1 _−_ _j_ 1 _, c_ 2 _−_ _j_ 2 _, · · ·, cr −_ _jr_ )

_j_ _[r]_ : _ji≤ci_

= _g_ ( _c_ 1 _, c_ 2 _, · · ·, cr_ ) _−_ E( _g_ ( _c_ 1 _−_ _J_ 1 _, c_ 2 _−_ _J_ 2 _, · · ·, cr −_ _Jr_ )) _,_

where _J_ _[r]_ is a multivariate hypergeometric vector with parameters ( _n_ 1 _, n_ 2 _, · · ·, nr, βtd_ ), where:



_i_ _[′]_ _<i_ _[c][i][′][ ≤]_ _[s][ ≤]_ [�]



_ci_ if

[�]



_ni_ =








_ci_ if [�] _i_ _[′]_ _≤i_ _[c][i][′][ ≤]_ _[s,]_

_s −_ [�] _[′]_ _[c][i][′]_ if [�] _[′]_ _[c][i][′][ ≤]_ _[s]_



_s −_ [�] _i_ _[′]_ _<i_ _[c][i][′]_ if [�] _i_ _[′]_ _<i_ _[c][i][′][ ≤]_ _[s][ ≤]_ [�] _i_ _[′]_ _≤i_ _[c][i][′]_ _._ (7)

0 otherwise







_i_ _[′]_ _<i_ _[c][i][′]_ if [�]



To explain the last equation, recall that _FFid_ [+] first ranks the elements of the input based on
Shap values, and chooses the top _s_ element. Then, it randomly and uniformly samples _βtd_ elements from the top _s_ elements (as long as _⌊αorig_ [+] _[s][⌋]_ _[> βtd]_ [). So, if the] _[ i]_ [-th influence tier satisfies]


15


Published as a conference paper at ICLR 2025





_i_ _[′]_ _≤i_ _[c][i][′][ ≤]_ _[s]_ [, then all of its elements will be chosen among the top] _[ s]_ [ influential elements. If]

- [�] [�]







_i_ _[′]_ _<i_ _[c][i][′][ ≤]_ _[s][ ≤]_ [�]



_i_ _[′]_ _≤i_ _[c][i][′]_ [, then] _[ s][ −]_ [�]





_i_ _[′]_ _<i_ _[c][i][′][ ≤]_ _[s][ ≤]_ [�] _i_ _[′]_ _≤i_ _[c][i][′]_ [, then] _[ s][ −]_ [�] _i_ _[′]_ _<i_ _[c][i][′]_ [ elements from the tier would be chosen among the]

top _s_, and otherwise, no elements will be chosen from the tier. So, in general, _ni_ elements from
each tier will be chosen, where _ni_ is defined in equation 7. Next, _FFid_ samples _βtd_ of these top _s_
elements and masks them. Thus, the number of elements masked in each tier follows a multivariate
hypergeometric distribution, modeling the sampling of _βtd_ elements from _n_ 1 _, n_ 2 _, · · ·, nr_ elements
of types 1 _,_ 2 _, · · ·, r_, respectively. As _s_ increases, the choice is _diluted_ by the inclusion of the lowest
tier elements in the top _s_ (since only _ni_ for the tier with [�] _i_ _[′]_ _<i_ _[c][i][′][ ≤]_ _[s][ ≤]_ [�] _i_ _[′]_ _≤i_ _[c][i][′]_ [ increases as] _[ s]_ [ is]



tier elements in the top _s_ (since only _ni_ for the tier with [�] _i_ _[′]_ _<i_ _[c][i][′][ ≤]_ _[s][ ≤]_ [�] _i_ _[′]_ _≤i_ _[c][i][′]_ [ increases as] _[ s]_ [ is]

increased to _s_ + 1). So, _J_ _[r]_ is decreasing (in terms of lexicographic ordering) as a function of _s_, and
_c_ _[r]_ _−_ _J_ _[r]_ is increasing as a function of _s_ . Since _e_ ( _s_ ) is linearly related to E( _g_ ( _c_ _[r]_ _−_ _J_ _[r]_ )), it follows
by the fact that _g_ ( _j_ 1 _, j_ 2 _, · · ·, jr_ ) is increasing with respect to the lexicographic ordering that _e_ ( _s_ ) is
decreasing in _s_ .
**Case 2:** _s ∈_ [0 _, c_ 1]
The proof follows by similar arguments as in the previous case. We have:
_e_ ( _s_ ) = _g_ ( _c_ 1 _, c_ 2 _, · · ·, cr_ ) _−_ _g_ ( _c_ 1 _−⌊α_ [+] _s⌋,_ 0 _,_ 0 _, · · ·,_ 0) _,_
It follows by the fact that _g_ ( _j_ 1 _, j_ 2 _, · · ·, jr_ ) is increasing with respect to the lexicographic ordering
that _e_ ( _s_ ) is increasing in _s_ .



C DETAILED EXPERIMENT SETUP


In F-Fidelity, the hyperparameter _β_ is used in both fine-tuning and evaluation. We select 0 _._ 1 by
default. For the sampling number, _N_, a larger number helps reduce the standard deviation but leads
to increasing evaluation time. In this paper, we select a balance point of 50. The hyperparameter _α_ [+]
and _α_ _[−]_ are designed to alleviate the OOD problem, we choose _α_ = _α_ [+] = _α_ _[−]_ = 0 _._ 5. We provide
hyperparameter sensitivity studies in Section D.9 and show comprehensive results in Section E.4.
For the key parameter _β_, we further provide an in-depth analysis in Section D.3. For baselines, we
use their default values as suggested in their original papers or codes.


We explore two architectures, ResNet and ViT for the image classification tasks. We use ResNet9 [3] and a 6-layer ViT as backbones for both CIFAR-100 and Tiny-Imagenet . In the ResNet
architecture, the hidden dimensions are set to [64 _,_ 128 _,_ 128 _,_ 256 _,_ 512 _,_ 512 _,_ 512 _,_ 512]. For the ViT
model, we use a patch size of 4, with a patch embedding dimension of 128. The backbone consists
of 6 attention layers, each with 8 heads for multi-head attention. The final output hidden dimension
of the ViT encoder is 256. In the training stage, we set the learning rate and weight decay to 1E-4
for ResNet and set the learning rate to 1E-3 and weight decay to 1E-4 for ViT. We use Adam as the
optimizer and the training epochs are 100 for ResNet and 200 for ViT. During fine-tuning, we use
the same hyperparameters.


For the time series classification task, we follow the TimeX (Queen et al., 2023) to use a simple
LSTM for both Boiler and PAM datasets. The simple LSTM model contains 3 bidirectional LSTM
layers with 128 hidden embedding sizes. We use AdamW as the optimizer with a learning rate of
1E-3 and weight decay is 1E-2.


For the natural language task, we use LSTM (Hochreiter & Schmidhuber, 1997) and Transformer
as the backbone. the LSTM has one hidden layer with the dimension set to 128. In Transformer,
the hidden dimension is set to 512. The number of Transformer layers for SST2 and BoolQ are 2
and 4. The head of the Transformer layers for SST2 and BoolQ are 4 and 8. For all datasets and
architectures, we use Adam optimizer (Kingma & Ba, 2015) with default learning rate 1E-4, training
epochs 100.


D EXTRA EXPERIMENTAL STUDY


D.1 EXPERIMENTS WITH OTHER BACKBONES.


**ViT as Backbone for Image Classification.** To further validate the robustness of our FFidelity framework across different model architectures, we conducted additional experiments using ViT (Dosovitskiy et al., 2020) as the backbone for image classification on both CIFAR-100 and


3https://jovian.com/tessdja/resnet-practice-cifar100-resnet


16


Published as a conference paper at ICLR 2025


TinyImageNet datasets. We utilize the SG-SQ (Smilkov et al., 2017) to generate explanations. For
the settings, we follow our main experimental setup. As shown in Table 5, F-Fidelity consistently
achieves best correlations across all metrics for both datasets, outperforming other methods. This
strong performance on ViT models further emphasizes the versatility and effectiveness of F-Fidelity
in evaluating explanations across different deep learning architectures.


Table 5: Spearman rank correlation results on CIFAR-100 and Tiny-Imagenet dataset with ViT.

|Correlation CIFAR-100|Tiny-Imagenet|
|---|---|
|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.94_±_0.00<br>-0.94_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.94_±_0.08<br>-0.94_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|-0.82_±_0.02<br>-0.83_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>-0.82_±_0.02<br>-0.83_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.68_±_0.40<br>-0.91_±_0.07<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.01<br>0.94_±_0.21<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.69_±_0.39<br>-0.77_±_0.44<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00|-0.74_±_0.27<br>-0.76_±_0.10<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>-0.74_±_0.27<br>-0.76_±_0.10<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>3.47_±_0.94<br>3.16_±_0.67<br>1.05_±_0.22<br>**1.00**_±_0.00<br>LeRF vs GT_ ↓_<br>1.16_±_0.36<br>**1.11**_±_0.31<br>1.32_±_0.57<br>1.32_±_0.57<br>MoRF vs LeRF_ ↓_<br>3.58_±_0.75<br>3.16_±_0.67<br>1.05_±_0.22<br>**1.00**_±_0.00|3.26_±_0.44<br>3.63_±_0.48<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>3.26_±_0.44<br>3.63_±_0.48<br>**1.00**_±_0.00<br>**1.00**_±_0.00|



**CNN as Backbone for Time Series Classification.** To verify the effectiveness of F-Fidelity on
Time Series, we conduct experiments with CNN as the classifier by following the setting in Section
5.2. As Table 6 shows, F-Fidelity consistently outperforms baselines across all metrics including
macro and micro correlations. Similarly, we observe the same phenomenon as shown in Table 3, the
Fidelity and ROAR suffer OOD problem. Under some sparsity, the correlation is not available for
the same rank while F-Fidelity and R-Fidelity perform well. These results on CNN models highlight
the versatility and effectiveness of F-Fidelity in assessing explanations across various deep learning
architectures.


Table 6: Spearman ranks correlations and ranks on time series datasets with CNN as the classifier.
The best performance is marked as bold. The “-” means the correlation can’t be obtained because
the different explanations have the same rank.

|Correlation PAM|Boiler|
|---|---|
|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|Fidelity<br>ROAR<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>0.78_±_0.10<br>-0.90_±_0.06<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>0.96_±_0.14<br>-0.71_±_0.10<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>0.68_±_0.18<br>**0.61**_±_0.11<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>0.91_±_0.07<br>0.86_±_0.07<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>-0.91_±_0.07<br>-0.86_±_0.07<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.02_±_0.62<br>-0.39_±_0.66<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>0.93_±_0.15<br>0.68_±_0.53<br>**1.00**_±_0.01<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.03_±_0.58<br>-0.12_±_0.61<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|-<br>-<br>**-0.94**_±_0.13<br>-0.92_±_0.18<br>-<br>-<br>**0.93**_±_0.11<br>**0.93**_±_0.11<br>-<br>-<br>-0.93_±_0.12<br>**-0.94**_±_0.11|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>3.00_±_1.26<br>2.00_±_1.08<br>**1.00**_±_0.00<br>1.11_±_0.45<br>LeRF vs GT_ ↓_<br>2.68_±_1.22<br>2.84_±_1.42<br>1.00_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>3.68_±_0.46<br>2.79_±_1.15<br>**1.00**_±_0.00<br>1.11_±_0.45|3.16_±_0.87<br>2.63_±_0.87<br>**1.11**_±_0.31<br>1.32_±_0.46<br>3.11_±_0.64<br>3.53_±_0.50<br>1.47_±_0.60<br>**1.21**_±_0.41<br>3.21_±_0.41<br>3.05_±_0.22<br>1.32_±_0.46<br>**1.16**_±_0.36|



**Transformer as Backbone for Natural Language Classification.** To further validate our findings
with modern architectures, we conducte experiments using Transformer models on both SST2 and
BoolQ datasets. As shown in Table 7, the results demonstrate interesting patterns. On SST2, while
all methods perform well, F-Fidelity shows slight advantages, particularly in micro correlations and
rankings. For BoolQ, all methods achieve nearly perfect correlations, suggesting that Transformer
models on this dataset exhibit strong robustness to perturbations, making the differences between
evaluation methods less pronounced. These results align with our earlier observations about the
inherent robustness of NLP models, while still demonstrating the reliability and slight advantages of
F-Fidelity in more challenging scenarios.


D.2 RELATIONSHIP BETWEEN MODEL ROBUSTNESS WITH F-FIDELITY PERFORMANCE


To understand how model robustness influences F-Fidelity’s effectiveness, we conduct a systematic evaluation across three domains: computer vision (CIFAR-100), time series (Boiler), and NLP
(SST2). For each domain, we test multiple architectures - ResNet and Transformer for CIFAR-100,
LSTM and CNN for Boiler, and LSTM and Transformer for SST2. We measure model robust

17


Published as a conference paper at ICLR 2025


Table 7: Spearman rank correlation and rank results with Transformer model on SST2 and BoolQ

|tasets. Correlation SST2|BoolQ|
|---|---|
|Fidelity<br>R-Fidelity<br>F-Fidelity|Fidelity<br>R-Fidelity<br>F-Fidelity|
|Macro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.94_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>LeRF vs GT_ ↑_<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>MoRF vs LeRF_ ↓_<br>-0.94_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Corr.<br>MoRF vs GT_ ↓_<br>-0.93_±_0.07<br>-0.97_±_0.05<br>**-1.00**_±_0.01<br>LeRF vs GT_ ↑_<br>0.98_±_0.04<br>**0.99**_±_0.02<br>**0.99**_±_0.03<br>MoRF vs LeRF_ ↓_<br>-0.92_±_0.09<br>-0.97_±_0.05<br>**-0.99**_±_0.02|**-1.00**_±_0.01<br>**-1.00**_±_0.01<br>**-1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00<br>**-1.00**_±_0.00|
|Micro<br>Rank<br>MoRF vs GT_ ↓_<br>2.05_±_0.89<br>1.32_±_0.46<br>**1.05**_±_0.22<br>LeRF vs GT_ ↓_<br>**1.16**_±_0.49<br>1.26_±_0.55<br>**1.16**_±_0.36<br>MoRF vs LeRF_ ↓_<br>2.05_±_0.97<br>1.32_±_0.46<br>**1.11**_±_0.45|**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00<br>**1.00**_±_0.00|



Table 8: Classification accuracy (%) comparison between original and fine-tuned models under
different perturbation ratios.






|Dataset Architecture Model 0% 5% 10%|Architecture|Model|0% 5% 10%|
|---|---|---|---|
|CIFAR-100<br>ResNet<br>Original<br>73.23<br>56.70<br>41.81<br>Fine-tuned<br>73.10<br>73.12<br>72.97<br>ViT<br>Original<br>50.25<br>46.19<br>39.87<br>Fine-tuned<br>46.91<br>47.05<br>47.30|ResNet|Original<br>Fine-tuned|73.23<br>56.70<br>41.81<br>73.10<br>73.12<br>72.97|
|CIFAR-100<br>ResNet<br>Original<br>73.23<br>56.70<br>41.81<br>Fine-tuned<br>73.10<br>73.12<br>72.97<br>ViT<br>Original<br>50.25<br>46.19<br>39.87<br>Fine-tuned<br>46.91<br>47.05<br>47.30|ViT|Original<br>Fine-tuned|50.25<br>46.19<br>39.87<br>46.91<br>47.05<br>47.30|


|CIFAR-100 ResNet Original 73.23 56.70 41.81 Fine-tuned 73.10 73.12 72.97 ViT Original 50.25 46.19 39.87 Fine-tuned 46.91 47.05 47.30|ResNet ViT|Original Fine-tuned Original Fine-tuned|73.23 56.70 41.81 73.10 73.12 72.97 50.25 46.19 39.87 46.91 47.05 47.30|
|---|---|---|---|
|Boiler<br>LSTM<br>Original<br>80.14<br>67.78<br>61.53<br>Fine-tuned<br>77.22<br>72.64<br>73.19<br>CNN<br>Original<br>82.22<br>68.89<br>57.92<br>Fine-tuned<br>86.39<br>74.03<br>63.89|LSTM|Original<br>Fine-tuned|80.14<br>67.78<br>61.53<br>77.22<br>72.64<br>73.19|
|Boiler<br>LSTM<br>Original<br>80.14<br>67.78<br>61.53<br>Fine-tuned<br>77.22<br>72.64<br>73.19<br>CNN<br>Original<br>82.22<br>68.89<br>57.92<br>Fine-tuned<br>86.39<br>74.03<br>63.89|CNN|Original<br>Fine-tuned|82.22<br>68.89<br>57.92<br>86.39<br>74.03<br>63.89|


|Boiler LSTM Original 80.14 67.78 61.53 Fine-tuned 77.22 72.64 73.19 CNN Original 82.22 68.89 57.92 Fine-tuned 86.39 74.03 63.89|LSTM CNN|Original Fine-tuned Original Fine-tuned|80.14 67.78 61.53 77.22 72.64 73.19 82.22 68.89 57.92 86.39 74.03 63.89|
|---|---|---|---|
|SST2<br>LSTM<br>Original<br>82.45<br>80.28<br>79.82<br>Fine-tuned<br>82.68<br>80.16<br>79.24<br>Transformer<br>Original<br>80.85<br>77.29<br>73.28<br>Fine-tuned<br>80.73<br>81.08<br>78.90|LSTM|Original<br>Fine-tuned|82.45<br>80.28<br>79.82<br>82.68<br>80.16<br>79.24|
|SST2<br>LSTM<br>Original<br>82.45<br>80.28<br>79.82<br>Fine-tuned<br>82.68<br>80.16<br>79.24<br>Transformer<br>Original<br>80.85<br>77.29<br>73.28<br>Fine-tuned<br>80.73<br>81.08<br>78.90|Transformer|Original<br>Fine-tuned|80.85<br>77.29<br>73.28<br>80.73<br>81.08<br>78.90|



ness through classification accuracy under different perturbation levels (0%, 5%, and 10%), where
perturbations are implemented through domain-specific masking operations: zeroing pixels for images, masking feature values for time series, and replacing token with zeros for text. For each
model-architecture pair, we compare the performance between the original model and its fine-tuned
version under these perturbation conditions.


Table 8 reveals distinct robustness patterns across different domains and architectures. In computer vision, models exhibit high sensitivity to perturbations - the original ResNet on CIFAR-100
shows dramatic accuracy degradation under even small perturbations, while its fine-tuned counterpart maintains stable performance. Time series models demonstrate moderate sensitivity, with original models showing notable but less severe accuracy drops under perturbations, which fine-tuning
helps to stabilize. In contrast, NLP models display inherent robustness, maintaining relatively stable
performance even under perturbations regardless of fine-tuning.


These robustness patterns directly correlate with F-Fidelity’s effectiveness. The performance improvements provided by F-Fidelity are most pronounced in domains where models show high sensitivity to perturbations (computer vision and time series). This is because traditional fidelity metrics in these domains suffer from unreliable evaluations due to OOD inputs, which our fine-tuning
strategy effectively mitigates. However, in NLP tasks where models possess natural robustness to
perturbations, the OOD problem is less severe, consequently diminishing the relative advantages of
F-Fidelity. This relationship provides practical guidance for choosing evaluation metrics - F-Fidelity
should be preferred in domains where models show high sensitivity to perturbations, while simpler
metrics may suffice for naturally robust domains.


18


Published as a conference paper at ICLR 2025


D.3 ANALYSIS OF DECISION BOUNDARY PRESERVATION UNDER RANDOM MASKING
FINE-TUNING


This section addresses whether fine-tuning the classification model with random masking (up to
fraction _β_ of input elements) significantly changes the model’s decision boundaries. We examine
this by looking at both global classification patterns and local decision characteristics. We consider
two aspects of the model’s decision boundaries as follows.


- **Global Decision Boundary** : The global decision boundary refers to the overall separation between different classes in the input space. It captures the main structure of the data distribution
and represents the classifier’s ability to distinguish between classes on a broad scale.

- **Local Decision Boundary** : The local decision boundary characterizes how the model responds
to small perturbations around individual data points.


(a) Classification accuracy (b) Prediction agreement with original prediction


Figure 1: Effects of random masking fine-tuning on model behavior. Higher fine-tuning _β_ values
improve robustness to perturbations but reduce agreement with the original model’s predictions.


Our random masking fine-tuning aims to maintain the model’s global classification behavior while
improving its stability to small input changes. The random masking process applies uniform noise
across all inputs, regardless of their class. This helps the model learn to handle missing information
while keeping its main classification patterns intact. Random masking fine-tuning maintains model
performance through several key principles. First, the random masking operation applies a uniform
noise distribution across the input space. Since this noise is class-agnostic and uniformly distributed,
it preserves the statistical properties of the underlying class distributions. Second, random masking
acts as a form of dropout regularization (Srivastava et al., 2014), which has been shown to improve model generalization while maintaining core decision boundaries. Third, the introduction of
masked inputs during training encourages the model to develop smoother decision boundaries in
local neighborhoods, as demonstrated by Bishop (1995) in the context of noise injection.


To empirically evaluate random masking fine-tuning, we conduct experiments with ResNet on
CIFAR-100. We examine both global decision boundary preservation and local boundary characteristics through systematic perturbation analysis. For global boundary preservation, we compare classification accuracy between the original and fine-tuned models on clean test data, using 0% masking
as the baseline reference point. We also measure prediction agreement between these models to
assess consistency in their decision-making. To analyze local boundary characteristics, we apply incremental perturbations during testing by randomly masking input elements, with perturbation sizes
ranging from 0.05 to 0.50. At each perturbation level, we track both classification accuracy and
prediction agreement with the original model. Classification accuracy reveals the model’s robustness to perturbations, while prediction agreement quantifies the consistency of decision boundaries
under noise. The masking ratio _β_ during fine-tuning controls the balance between preserving global
boundaries and smoothing local ones.


Our experimental results demonstrate that random masking fine-tuning effectively balances global
decision boundary preservation with local boundary smoothing. The fine-tuned models maintain


19


Published as a conference paper at ICLR 2025


comparable baseline accuracy to the original model while showing significantly improved robustness
to perturbations. On clean test data, the accuracy difference between original and fine-tuned models
remains small, indicating preserved global classification behavior. The high prediction agreement
between original and fine-tuned models on clean data further confirms the preservation of global
decision patterns. Meanwhile, the smoother local decision boundaries manifest through sustained
performance under perturbations, with fine-tuned models showing notably accuracy change compared to the original model.


The masking ratio _β_ plays a crucial role in model behavior under perturbations, as evidenced in
Figure 1. In terms of accuracy (Figure 1(a)), models fine-tuned with larger _β_ values demonstrate
remarkable resilience, maintaining accuracy above 60% even under 50% input perturbation, while
the original model ( _β_ = 0) drops below 40% accuracy with just 10% perturbation. The prediction
agreement results (Figure 1(b)) show a trade-off. Higher _β_ values lead to better accuracy under
perturbations and result in lower agreement with the original model’s predictions. This suggests that
stronger fine-tuning with larger _β_ values causes the model to learn more robust but slightly different
decision boundaries compared to the original model. Models with moderate _β_ values (like 0.10)
maintain a better balance between perturbation resistance and consistency with the original model’s
behavior.


D.4 PRESERVATION OF EXPLANATION AFTER FINE-TUNING


To further validate that our fine-tuning strategy preserves meaningful explanations, we conduct a
qualitative analysis on Tiny-Imagenet. We visualize and compare the attribution maps generated
for both the original and fine-tuned models using GradCAM. As shown in Table 9, the attribution
maps from both models highlight similar regions of importance, with only minor variations in the
highlight intensity. This visual consistency suggests that while our fine-tuning process improves
model robustness to perturbations, it maintains the model’s original attention to meaningful features,
thereby preserving the validity of the generated explanations.


Table 9: The explanation visualization comparison between original and fne-tuned model.


ResNet ViT


Input Original Fine-tuned Original Fine-tuned


20


Published as a conference paper at ICLR 2025


D.5 EVALUATION WITH LAYERCAM AS ALTERNATIVE EXPLANATION METHOD


To further validate the robustness of our method across different explanation techniques, we conduct
additional experiments using LayerCAM (Jiang et al., 2021), an alternative explanation method to
GradCAM. Following the same experimental setup as in Table 1, we evaluate the performance using
ResNet architecture on the CIFAR-100 dataset. As shown in Table 10, F-Fidelity maintains its superior performance with LayerCAM, exhibiting consistent patterns with our GradCAM results. This
consistency across different explanation methods reinforces the generalizability of our evaluation
framework and its ability to assess various types of explanation techniques reliably.


Table 10: Spearman rank correlation and rank results on CIFAR-100 dataset with ResNet explained
by LayerCAM.


Correlation Fidelity ROAR R-Fidelity F-Fidelity


MoRF vs GT _↓_ 0.77 _±_ 0.06 0.98 _±_ 0.03 0.26 _±_ 0.00 **-0.26** _±_ 0.00
LeRF vs GT _↑_ **1.00** _±_ 0.00 **1.00** _±_ 0.00 **1.00** _±_ 0.00 **1.00** _±_ 0.00
MoRF vs LeRF _↓_ 0.77 _±_ 0.06 0.98 _±_ 0.03 0.26 _±_ 0.00 **-0.26** _±_ 0.00


MoRF vs GT _↓_ 0.53 _±_ 0.40 0.81 _±_ 0.20 -0.02 _±_ 0.22 **-0.50** _±_ 0.61
LeRF vs GT _↑_ 0.99 _±_ 0.02 0.99 _±_ 0.04 **1.00** _±_ 0.00 **1.00** _±_ 0.00
MoRF vs LeRF _↓_ 0.53 _±_ 0.41 0.81 _±_ 0.20 -0.02 _±_ 0.22 **-0.50** _±_ 0.61


MoRF vs GT _↓_ 2.95 _±_ 0.23 3.95 _±_ 0.23 1.89 _±_ 0.66 **1.21** _±_ 0.42
LeRF vs GT _↓_ 2.53 _±_ 1.39 1.95 _±_ 1.18 1.05 _±_ 0.23 **1.00** _±_ 0.00
MoRF vs LeRF _↓_ 2.89 _±_ 0.32 3.95 _±_ 0.23 1.89 _±_ 0.56 **1.21** _±_ 0.42


D.6 CASE STUDY


To provide the intuitional verification of our method, we conduct a case study with an image from the
Tiny-Imagenet dataset. We first use GradCAM to obtain the explanation. To get various degraded
explanations, we follow the previous setting to add noise, which is a list of [0 _._ 2 _,_ 0 _._ 4 _,_ 0 _._ 6 _,_ 0 _._ 8 _,_ 1 _._ 0],
to the explanation to obtain the ground truth ranking of explanations. We compare baselines and
F-Fidelity and show the results in Figure 2. We observe that the Fidelity and ROAR methods fail
to differentiate explanations with noise levels of 0.2 and 0.4 when the sparsity is below certain
thresholds (e.g., 25%). However, the R-Fidelity and F-Fidelity have a good performance in such
cases and F-Fidelity has a better performance than R-Fidelity from the micro correlation results.


D.7 ABLATION STUDY


To evaluate the effectiveness of our proposed F-Fidelity framework, we conduct ablation studies comparing three evaluation setups: the original Fidelity metric, Fidelity with fine-tuning
(Fidelity+Fine-tune), and F-Fidelity. We utilize the CIFAR-100 dataset for these experiments, employing a ResNet architecture as the backbone for the classifier. The explainers are based on SG-SQ.


As shown in Figure 3, across all three correlation metrics—“MoRF vs GT”, “LeRF vs GT”, and
“MoRF vs LeRF”—F-Fidelity consistently outperforms the other methods. Notably, Fidelity+Finetune shows a marked improvement over the original Fidelity, indicating that the fine-tuning step
enhances the robustness of the evaluation. F-Fidelity further advances this by effectively addressing
the OOD issue, resulting in the most faithful and reliable rankings of the explainers.


D.8 EMPRICAL VERIFICATION OF DETERMINING THE EXPLANATION SIZE


In this section, we demonstrate empirically that the _FFid_ metric can be used to deduce the size
or sparsity of explanations, complementing the theoretical analysis of Section 4. Specifically, we
consider the colored-MNIST dataset (Arjovsky et al., 2019), where the explanation corresponds to
the pixels representing the digit in each image. To control the explanation size, we rescale and crop
the image samples so that the digit pixels occupy _γ ∈{_ 0 _._ 1 _,_ 0 _._ 15 _,_ 0 _._ 2 _,_ 0 _._ 25 _}_ fraction of the total
image area. Our goal is to show empirically that the _FFid_ metric can recover the value of _γ_, thus
revealing the explanation size. A three-convolution-layer model is used as the pre-trained model.


21


Published as a conference paper at ICLR 2025


Figure 2: Case study of a sample in Tiny-Imagenet. We use GradCAM to obtain the explanation
and get degraded explanations by adding random noise to the explanation. The noise level is a list
of [0 _._ 2 _,_ 0 _._ 4 _,_ 0 _._ 6 _,_ 0 _._ 8 _,_ 1 _._ 0]. We visualize the Fidelity score and Correlation on different sparsity.


(a) MoRF vs GT (b) LeRF vs GT (c) MoRF vs LeRF


Figure 3: The ablation study between ground truth explanation size _γ_ and _FFid_ [+] in F-Fidelity.


Following the terminology of Section 4, the pixels can be divided into two tiers: those belonging
to the digit (forming the explanation) and those in the background. The explanation size is thus
_c_ 1 = _γtd_ . According to Theorem 1, we expect _FFid_ [+] to increase for _s < c_ 1 and decrease for
_s >_ max - _αβ_ [+] _[td, γtd]_ �.


To verify this behavior, we evaluate _FFid_ [+] across three settings of _α_ [+] and five settings of _β_ . As
shown in Figure 4, the ground truth explanation size can be identified from the flat area in _RFid_ [+]
curves. Two critical points emerge on the x-axis: the explanation size _γ_ and the point where the
removal attributions equal the explanation size ( _β/α_ [+] ). The region between these points forms a
plateau that enables reliable comparison of explanations. When _αβ_ [+] _[td < c]_ [1][, the point where] _[ FFid]_ [+]
changes direction corresponds to the explanation size _γ_, allowing us to recover the true explanation
size by evaluating _FFid_ [+] over different _β_ values.


22


Published as a conference paper at ICLR 2025


While our theoretical analysis assumes uniform importance weights across explanation components,
real-world applications show some deviation from this idealized behavior, manifesting as smooth
transitions rather than sharp changes in the _FFid_ [+] curves. Nevertheless, the empirical results validate the theoretical predictions and demonstrate the utility of _FFid_ metrics in determining explanation sizes.


(a) _γ_ = 0 _._ 1 _, α_ [+] = 0 _._ 3 (b) _γ_ = 0 _._ 15 _, α_ [+] = 0 _._ 3 (c) _γ_ = 0 _._ 2 _, α_ [+] = 0 _._ 3 (d) _γ_ = 0 _._ 25 _, α_ [+] = 0 _._ 3


(e) _γ_ = 0 _._ 1 _, α_ [+] = 0 _._ 5 (f) _γ_ = 0 _._ 15 _, α_ [+] = 0 _._ 5 (g) _γ_ = 0 _._ 2 _, α_ [+] = 0 _._ 5 (h) _γ_ = 0 _._ 25 _, α_ [+] = 0 _._ 5


(i) _γ_ = 0 _._ 1 _, α_ [+] = 0 _._ 7 (j) _γ_ = 0 _._ 15 _, α_ [+] = 0 _._ 7 (k) _γ_ = 0 _._ 2 _, α_ [+] = 0 _._ 7 (l) _γ_ = 0 _._ 25 _, α_ [+] = 0 _._ 7


Figure 4: The relationship between ground truth explanation size _γ_ and _RFid_ [+] in F-Fidelity. We
compare different _α_ [+] and _β_ in the evaluation stage. We observe that the ground truth can be inferred
from the _RFid_ [+] .


D.9 HYPERPARAMETER SENSITIVITY STUDY


In this part, we assess the robustness of F-Fidelity to hyperparameter choices. We select a subset of
400 samples with an explanation size ratio of 0.2 from the colored-MNIST dataset (Arjovsky et al.,
2019). we have three key hyperparameters: the number of sampling iterations _N_, the ratio _α_ [+] _/α_ _[−]_,
and _β_ . For each hyperparameter, we explored a range of values to understand their impact on the
performance of F-Fidelity. We evaluated the method’s performance using three Macro Spearman
correlations, with additional detailed results on _RFid_ [+], _RFid_ _[−]_, and Micro Spearman correlations
provided in Appendix E.4.


Figure 5 presents the results of our hyperparameter sensitivity study. The findings demonstrate that
F-Fidelity exhibits remarkable stability across a wide range of hyperparameter settings. This consistency is observed across all three Macro Spearman correlations, indicating that the performance
of F-Fidelity is not overly sensitive to specific hyperparameter choices.


D.10 COMPARING DIFFERENT EXPLANATION METHODS


To demonstrate F-Fidelity’s ability to differentiate between explanation methods of varying quality,
we conduct a comparative study between GradCAM and Saliency Map (Simonyan et al., 2013)
on both Tiny-Imagenet and ImageNet. GradCAM typically produces more focused explanations
by utilizing class-specific gradient information at higher-level feature maps, while Saliency Map
operates directly on pixel-level gradients.


23


Published as a conference paper at ICLR 2025


(a) Ablation study on _N_ (b) Ablation study on _β_ (c) Ablation study on _α_


Figure 5: Ablation study on sampling number _N_, _β_, and _α_ = _α_ [+] = _α_ _[−]_ . We report the Macro
Spearman rank correlations.


Our evaluation on Tiny-Imagenet confirms this expected difference. Across our test set, GradCAM
achieves better performance with higher _FFid_ [+] (0.2286 vs 0.2020) and lower _FFid_ _[−]_ (0.0547
vs 0.1050) scores than Saliency Map. Note that both explanation methods are used to generate
explanations with respect to the ground truth label. Thus, the FFid values are negative if the original
prediction is incorrect.


Table 11: Comparative evaluation of GradCAM and Saliency Map explanations using F-Fidelity
on Tiny-Imagenet. We show five cases with their original images and corresponding explanation
visualizations. The _FFid_ [+] and _FFid_ _[−]_ scores are averaged over different sparsity levels, with bold
scores indicating better performance. The _FFid_ [+] and _FFid_ _[−]_ scores of case 5 are negative because
the original image is misclassified. The bottom row reports metrics averaged across all samples in

|est set.|Col2|Col3|
|---|---|---|
|<br>Image<br>Explanation Visualization<br>Evaluation Metrics<br>Original<br>GradCAM<br>Saliency Map<br>Method<br>_FFid_+ _↑_<br>_FFid−↓_|Explanation Visualization|Evaluation Metrics<br>|
|<br>Image<br>Explanation Visualization<br>Evaluation Metrics<br>Original<br>GradCAM<br>Saliency Map<br>Method<br>_FFid_+ _↑_<br>_FFid−↓_|Original<br>GradCAM<br>Saliency Map|Method<br>_FFid_+ _↑_<br>_FFid−↓_|
|Case 1||GradCAM<br>Saliency Map<br>**0.8743**<br>0.8638<br>**0.1267**<br>0.3342|
|Case 2||GradCAM<br>Saliency Map<br>**0.3838**<br>0.3143<br>**0.029**<br>0.0048|
|Case 3||GradCAM<br>Saliency Map<br>**0.2095**<br>0.0752<br>**0.0190**<br>0.0314|
|Case 4||GradCAM<br>Saliency Map<br>**0.4552**<br>0.1095<br>**0.0486**<br>0.1000|
|Case 5||GradCAM<br>Saliency Map<br>**-0.0057**<br>-0.0086<br>**-0.0048**<br>-0.0029|
|Average over All Images in The Test Set|Average over All Images in The Test Set|GradCAM<br>Saliency Map<br>**0.2286**<br>0.2020<br>**0.0547**<br>0.1050|



To verify our method’s effectiveness on high-dimensional data, we further evaluate on a subset of
ImageNet (5000 images, 224×224×3 resolution) using ResNet-18 as the backbone. Unlike our previous experiments where we fine-tuned on training data and evaluated explainers on a separate test
set, here we fine-tune and evaluate on the same image set to enable controlled comparison between
explainers. Note that this fine-tuned model is specifically optimized for this dataset and should
only be used to compare explainer performance within this set. Following the same protocol as our
Tiny-Imagenet experiments, we fine-tune the model using Adam optimizer with default settings and


24


Published as a conference paper at ICLR 2025


evaluate on 500 sampled images from this set. As shown in Table 12, The results consistently show
GradCAM outperforming Saliency Map, with higher _FFid_ [+] (0.1123 vs 0.0672) and lower _FFid_ _[−]_
(0.0046 vs 0.0091) scores. It shows that F-Fidelity successfully captures the qualitative differences
between explainers, such as GradCAM’s more focused attribution maps compared to Saliency Map’s
noisier explanations.


Table 12: Comparative evaluation of GradCAM and Saliency Map explanations using F-Fidelity
on ImageNet dataset with ResNet-18. The _FFid_ [+] and _FFid_ _[−]_ scores are averaged over different
sparsity levels, with bold scores indicating better performance.

|Explanation Visualization Evaluation Metrics<br>Image<br>Original GradCAM Saliency Map Method FFid+ ↑ FFid−↓|Explanation Visualization|Evaluation Metrics|
|---|---|---|
|Image<br>Explanation Visualization<br>Evaluation Metrics<br>Original<br>GradCAM<br>Saliency Map<br>Method<br>_FFid_+ _↑_<br>_FFid−↓_|Original<br>GradCAM<br>Saliency Map|Method<br>_FFid_+ _↑_<br>_FFid−↓_|
|Case 1||GradCAM<br>Saliency Map<br>**0.3476**<br>0.2362<br>**0.0019**<br>0.0057|
|Case 2||GradCAM<br>Saliency Map<br>**0.3981**<br>0.3571<br>**0.0076**<br>0.0105|
|Case 3||GradCAM<br>Saliency Map<br>**0.5438**<br>0.2162<br>**0.0257**<br>0.0590|
|Case 4||GradCAM<br>Saliency Map<br>**0.6190**<br>0.1019<br>**0.0029**<br>0.0162|
|Case 5||GradCAM<br>Saliency Map<br>**-0.0010**<br>-0.1477<br>**-0.2467**<br>-0.0895|
|Average Over 500 Images|Average Over 500 Images|GradCAM<br>Saliency Map<br>**0.1123**<br>0.0672<br>**0.0046**<br>0.0091|



D.11 COMPUTATIONAL EFFICIENCY ANALYSIS


Following the comparative setup between GradCAM and Saliency Map on Tiny-Imagenet, we conduct a detailed computational efficiency analysis. Table 13 breaks down the computational time
requirements for both ROAR and F-Fidelity. The key advantage of F-Fidelity lies in its one-time
fine-tuning cost - while ROAR requires separate explaining and training processes for each explainer
(totaling 7140s for two explainers), F-Fidelity needs only a single fine-tuning process (2925s) that
can be reused across all explainers.


For the Tiny-Imagenet dataset with ResNet backbone, ROAR’s per-explainer cost includes generating explanations for training samples (approximately 650-700s), model retraining (around 2900s),
and evaluation (about 120s). In contrast, F-Fidelity requires only a single fine-tuning step (2925s),
followed by evaluation for each explainer (approximately 550s). This translates to a total processing
time of 4026s for F-Fidelity compared to 7378s for ROAR when evaluating two explainers, with the
gap widening further as more explainers are evaluated.


This efficiency gain becomes particularly significant when evaluating multiple explainers or conducting extensive ablation studies, making F-Fidelity more practical for real-world applications
while maintaining superior evaluation quality as demonstrated in our previous experiments. The
advantage is even more pronounced for computationally intensive explainers, as ROAR requires


25


Published as a conference paper at ICLR 2025


Table 13: Computational time analysis for evaluating GradCAM and Saliency Map on TinyImagenet using ResNet as the backbone. While ROAR requires separate explaining and training
processes for each explainer, F-Fidelity’s fne-tuning cost is one-time.


Explaining Training Model Training/ Evaluation Total
Method
Samples (100k) Fine-tuning (100 epochs) (2000 samples) Time


ROAR 647s + 721s 2911s + 2861s 114s+124s 7378s
F-Fidelity   - 2925s 541s+560s 4026s


generating explanations for the entire training set (100k+ samples in our case), while F-Fidelity
only needs explanations for the evaluation set. Moreover, our hyperparameter analysis shows that
we can further reduce the evaluation time by decreasing the number of samples - with _N_ = 10,
the evaluation time per explainer drops from approximately 550s to 180s while maintaining robust
performance. This flexibility allows users to balance between evaluation speed and precision based
on their specific needs.


E DETAILED EXPERIMENTAL RESULTS


E.1 IMAGE CLASSIFICATION EXPLANATION EVALUATION


**Experiments on CIFAR-100** . We provide the detailed experiment results on the CIFAR-100
dataset. The CIFAR-100 dataset is a collection of 60,000 color images, each sized 32x32 pixels,
classified into 100 distinct categories, with 600 images per class. It contains 50,000 training images
and 10,000 test images. For the ResNet backbone, in Table 14, we compared different methods on
the _Fid_ + and _Fid−_ metrics. From the figures, we found R-Fidelity and F-Fidelity methods have
the advantage of distinguishing explanations with larger margin scores.


Similar results can be observed for the ViT backbone. We provide the results in Table 15.


Table 14: Fidelity results on CIFAR-100 dataset with ResNet as the model to be explained.


Fidelity ROAR R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


**Experiments on Tiny-Imagenet** . Similar to the CIFAR-100, we provide the detailed experiment
results on Tiny-Imagenet in this section, which is a scaled-down version of the ImageNet dataset,
designed for image classification tasks. Tiny-Imagenet contains 200 classes and 500 training images
per class, 50 validation images, and 50 test images per class. The image resolution is 64x64. From
Table 16 and 17, we observe that compared to R-Fidelity and F-Fidelity, F-Fidelity and R-Fidelity
have the best performance.


26


Published as a conference paper at ICLR 2025


Table 15: Fidelity results on CIFAR-100 dataset with ViT as the model to be explained.


Fidelity ROAR R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


Table 16: Fidelity results on Tiny-Imagenet dataset with ResNet as the model to be explained.


Fidelity ROAR R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


Table 17: Fidelity results on Tiny-Imagenet dataset with ViT as the model to be explained.


Fidelity ROAR R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


27


Published as a conference paper at ICLR 2025


E.2 DETAILED RESULTS ON TIME SERIES DATASETS


We conduct the experiments on the PAM and Boiler datasets with LSTM architecture. As Table 18
and 19 show, our method distinguishes explanations with a larger margin than R-Fidelity. Moreover,
Fidelity and ROAR method fail to distinguish different explanations in the Boiler dataset.


Table 18: Fidelity results on PAM dataset with LSTM as the model to be explained.


Fidelity ROAR R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


Table 19: Fidelity results on Boiler dataset with LSTM as the model to be explained.


Fidelity ROAR R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


E.3 DETAILED RESULTS ON NLP


In Table 20, 21, 22 and 23, with LSTM and Transformer, we find the Fidelity has a consistent
result which is quite different than other tasks. Compared to R-Fidelity and F-Fidelity, the results
are almost the same both in the SST2 and BoolQ datasets, which indicates in these two cases, the
OOD problem can be ignored.


28


Published as a conference paper at ICLR 2025


Table 20: Fidelity results on SST2 dataset with LSTM as the model to be explained.


Fidelity R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


Table 21: Fidelity results on SST2 dataset with Transformer as the model to be explained.


Fidelity R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


Table 22: Fidelity results on BoolQ dataset with LSTM as the model to be explained.


Fidelity R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


29


Published as a conference paper at ICLR 2025


Table 23: Fidelity results on BoolQ dataset with Transformer as the model to be explained.


Fidelity R-Fidelity F-Fidelity


_Fid_ [+]


_Fid_ _[−]_


30


Published as a conference paper at ICLR 2025


E.4 DETAILED RESULTS OF HYPERPARAMETER SENSITIVITY STUDY


In this section, we provide the detailed ablation study experiment results on the Colored-Mnist
dataset. The experiment settings are described as Section D.9. We provide the detailed _Fid_ [+],
_Fid_ _[−]_, and three Spearman rank Correlation, same as the subsection E.1. As Table 24, 25, and 26
show, our method is robust to the choice of hyperparameters.


Table 24: The detailed ablation study results on sampling number _N_, with _β_ = 0 _._ 1, _α_ [+] = _α_ _[−]_ = 0 _._ 5.


Sampling Local Spearman
_Fid_ [+] _Fid_ _[−]_
Number _N_ Correlation


10


30


50


80


150


31


Published as a conference paper at ICLR 2025


Table 25: The detailed ablation study results on hyperparameter _β_, with _N_ = 50, _α_ [+] = _α_ _[−]_ = 0 _._ 5.


Local Spearman
_β_ _Fid_ [+] _Fid_ _[−]_
Correlation


0.05


0.10


0.15


0.20


0.25


32


Published as a conference paper at ICLR 2025


Table 26: The detailed ablation study results on hyperparameter _α_ = _α_ [+] = _α_ _[−]_, with _N_ = 50,
_β_ = 0 _._ 1.


Local Spearman
_α_ _Fid_ [+] _Fid_ _[−]_
Correlation


0.2


0.3


0.4


0.5


0.6


0.7


0.8


33


